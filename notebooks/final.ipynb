{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abf53261",
   "metadata": {},
   "source": [
    "# Full Project Code\n",
    "\n",
    "The first part of this notebook is responsible for transforming the raw json data into a usable tabular format. Due to the robustness of XGBoost as an algorithm, we do not need to handle missing values, and can pass them directly as nulls.\n",
    "\n",
    "Note that we must have created the relevant csv files before running this code, which can be done using `scripts/convert_json_data.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308c59e5",
   "metadata": {},
   "source": [
    "## Setup\n",
    "This part of the notebook imports all dependencies, and sets up any global constants we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79552a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import xgboost as xg\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6248e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(\"/\", \"home\", \"walkerdavis\", \"projects\", \"mpsc\"))\n",
    "DATA_ROOT = os.path.join(\"data\", \"processed\")\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73a43b3",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "The `GrowthDataTransformation` class encapsulates all data loading and feature engineering for the model.\n",
    "\n",
    "### `__init__(self)`\n",
    "Initializes the class instance. Sets up placeholders for the consolidated output dataframe (`self.consolidated_data`) and a dictionary for patient-specific growth model interpolators (`self.growth_models`).\n",
    "\n",
    "### `load_and_process_demographics(self, demographics_path: str)`\n",
    "Reads the demographics CSV and cleans the resulting dataframe:\n",
    "- Converts Gestational Age from a string to numeric, and extracts the integer.\n",
    "- Parses DOB, and Data and Fluid Start and End as datetime objects.\n",
    "- Filters out records with future fluid dates, missing/invalid GA, or birth weight outliers.\n",
    "\n",
    "### `load_and_process_weight_data(self, weight_path: str):`\n",
    "Loads and cleans weight measurement data.\n",
    "- Converts weights to grams (assumes original is in ounces).\n",
    "- Adds a column marking these as weight measurements (`OBX`).\n",
    "- Converts timestamps to datetime.\n",
    "- Removes rows with missing weight values.\n",
    "\n",
    "### `create_growth_tibble(self, demographics, weight_data)`:\n",
    "Joins demographics and weight data, and computes time-relative variables.\n",
    "- Merges demographics and weight by `ID`.\n",
    "- Calculates days since birth for each weight entry.\n",
    "- Computes post-menstrual age (PMA) in days and weeks.\n",
    "\n",
    "### `fit_growth_models(self, growth_data, frac=0.3)`:\n",
    "Fits a smoothed weight curve for each patient and saves interpolation functions to the main class.\n",
    "- Filters patients with at least 3 weight observations.\n",
    "- For each, sorts by age, fits a LOESS smoothed curve to weight over time. NOTE: I had attemtped to use an alternative smoothing algorithm that did not need to reference future values, however I ran into issue where the algorithm would generate massive amounts of missing values later in the dataset.\n",
    "- Builds an interpolating function for each patient’s smoothed weights.\n",
    "- Stores models and returns all patient data with smoothed weights included.\n",
    "\n",
    "### `load_and_process_energy_data(self, energy_path: str)`:\n",
    "Loads and processes daily energy intake data.\n",
    "- Reads CSV, converts timestamps.\n",
    "- Marks whether energy was delivered parenterally (IV) or enterally.\n",
    "- Aggregates total energy per day and route.\n",
    "- Pivots into one row per patient per day, with enteral, parenteral, and total energy columns.\n",
    "\n",
    "### `load_and_process_pulse_data(self, pulse_path)`:\n",
    "Loads and processes pulse rate data.\n",
    "- Reads CSV, extracts date.\n",
    "- Aggregates mean daily pulse for each patient.\n",
    "\n",
    "### `load_and_process_spo2_data(self, spo2_path)`:\n",
    "Loads and processes daily oxygen saturation (SpO2) data.\n",
    "- Reads CSV, extracts date.\n",
    "- Aggregates mean daily SpO2 for each patient.\n",
    "\n",
    "### `load_and_process_stool_weight_data(self, stool_path)`:\n",
    "Loads and sums daily stool output.\n",
    "- Reads CSV, extracts date.\n",
    "- Aggregates total stool output per patient per day.\n",
    "\n",
    "### `load_and_process_bp_data(self, bp_path: str)`:\n",
    "Loads and processes blood pressure readings.\n",
    "- Reads CSV, parses dates.\n",
    "- Splits BP readings into systolic and diastolic values.\n",
    "- Aggregates mean daily systolic/diastolic BP per patient.\n",
    "\n",
    "### `load_and_process_resp_data(self, resp_path)`:\n",
    "Loads and processes daily respiratory rate data.\n",
    "- Reads CSV, extracts date.\n",
    "- Aggregates mean daily respiratory rate per patient.\n",
    "\n",
    "### `load_and_process_io_data(self, io_path)`:\n",
    "Loads and processes daily input/output data.\n",
    "- Reads CSV, parses date.\n",
    "- Pivots to one row per patient per day, with each OBX variable as a column.\n",
    "\n",
    "### `predict_smoothed_weight(self, patient_id, day_from_birth)`:\n",
    "For a given patient and day, returns the smoothed weight value using their fitted curve.\n",
    "- Looks up the interpolation model and evaluates at the requested day.\n",
    "\n",
    "### `calculate_weight_derivatives(self, patient_id, day_from_birth)`:\n",
    "Calculates the patient’s weight velocity (gain) on a specific day.\n",
    "- Uses smoothed weights to estimate daily and weekly gains via finite differences.\n",
    "\n",
    "### `calculate_rolling_statistics(self, df, patient_col, date_col, value_col, window=7)`:\n",
    "Adds rolling mean and standard deviation for a feature, per patient.\n",
    "- Sorts and groups by patient, computes rolling window stats for a given column.\n",
    "\n",
    "### `consolidate_all_data(self, ...)`:\n",
    "Merges all available data into a single, comprehensive table and computes derived features.\n",
    "- Joins all processed daily data on patient and date.\n",
    "- Computes key clinical time variables (days since birth, PMA).\n",
    "- Encodes sex.\n",
    "- Fills in smoothed weights and weight gains.\n",
    "- Calculates per-kg rates for energy, fluid, and other features.\n",
    "- Adds rolling stats and a categorical growth stage.\n",
    "- Filters to relevant PMA window.\n",
    "- Randomly splits patients into train/test groups.\n",
    "\n",
    "### `get_normal_weekly_growth(self, ga, week_from_birth, weight_g)`:\n",
    "Returns the expected (“normal”) weekly weight gain for a given gestational age and week.\n",
    "- Based on clinical conventions, uses fixed or proportional values by age.\n",
    "\n",
    "### `add_normal_growth_comparison(self)`:\n",
    "Appends a column for expected normal weekly growth to the consolidated data.\n",
    "- For each row, computes the expected growth using get_normal_weekly_growth.\n",
    "\n",
    "### `create_feature_list(self)`:\n",
    "Assemble a full feature list\n",
    "- Lists key base features (per-kg, etc.).\n",
    "- Optionally includes fluid intake/output and their rolling stats, if the base features are present.\n",
    "- Extends list to include squared and square root transformations.\n",
    "\n",
    "### `create_polynomial_features(self)`:\n",
    "Adds polynomial and root-transformed versions of all base features.\n",
    "- For each feature, adds a squared and square root column (if non-negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e164f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrowthDataTransformation:\n",
    "    def __init__(self):\n",
    "        self.consolidated_data = None\n",
    "        self.growth_models = {}\n",
    "\n",
    "    def load_and_process_demographics(self, demographics_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Load and process demographics data\"\"\"\n",
    "        demographics = pd.read_csv(demographics_path)\n",
    "\n",
    "        demographics[\"GA\"] = pd.to_numeric(\n",
    "            demographics[\"GA\"].str.extract(r\"(\\d+)\")[0], errors=\"coerce\"\n",
    "        )\n",
    "\n",
    "        demographics[\"DOB\"] = pd.to_datetime(demographics[\"DOB\"])\n",
    "        demographics[\"DataStart\"] = pd.to_datetime(demographics[\"DataStart\"])\n",
    "        demographics[\"DataEnd\"] = pd.to_datetime(demographics[\"DataEnd\"])\n",
    "        demographics[\"FluidStart\"] = pd.to_datetime(demographics[\"FluidStart\"])\n",
    "        demographics[\"FluidEnd\"] = pd.to_datetime(demographics[\"FluidEnd\"])\n",
    "\n",
    "        demographics = demographics[\n",
    "            (demographics[\"FluidStart\"] < pd.to_datetime(\"2200-01-01\"))\n",
    "            & (demographics[\"GA\"].notna())\n",
    "            & (demographics[\"GA\"] > 20)\n",
    "            & (demographics[\"BW\"].notna())\n",
    "            & (demographics[\"BW\"] > 200)\n",
    "        ]\n",
    "\n",
    "        return demographics\n",
    "\n",
    "    def load_and_process_weight_data(self, weight_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Load and process weight data\"\"\"\n",
    "        weight_data = pd.read_csv(weight_path)\n",
    "        assert isinstance(weight_data, pd.DataFrame)\n",
    "\n",
    "        weight_data[\"Value\"] = weight_data[\"Value\"] * 28.35\n",
    "        weight_data[\"OBX\"] = \"WeightG\"\n",
    "        weight_data[\"DateTime\"] = pd.to_datetime(weight_data[\"DateTime\"])\n",
    "\n",
    "        weight_data = weight_data.dropna(subset=[\"Value\"])\n",
    "\n",
    "        return weight_data[[\"ID\", \"DateTime\", \"OBX\", \"Value\"]]\n",
    "\n",
    "    def create_growth_tibble(\n",
    "        self, demographics: pd.DataFrame, weight_data: pd.DataFrame\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Join demographics and weight data: compute additional variables\"\"\"\n",
    "        growth_data = demographics[[\"ID\", \"DOB\", \"GA\", \"BW\", \"Sex\"]].merge(\n",
    "            weight_data, on=\"ID\", how=\"inner\"\n",
    "        )\n",
    "\n",
    "        growth_data[\"DayFromBirthNumeric\"] = (\n",
    "            growth_data[\"DateTime\"] - growth_data[\"DOB\"]\n",
    "        ).dt.days.astype(float)\n",
    "\n",
    "        growth_data[\"PMADays\"] = (\n",
    "            7 * growth_data[\"GA\"] + growth_data[\"DayFromBirthNumeric\"]\n",
    "        ).astype(int)\n",
    "\n",
    "        growth_data[\"PMAWeeks\"] = (growth_data[\"PMADays\"] / 7).astype(int)\n",
    "\n",
    "        return growth_data\n",
    "\n",
    "    def fit_growth_models(self, growth_data: pd.DataFrame, frac: float = 0.3):\n",
    "        patient_counts = growth_data.groupby(\"ID\").size()\n",
    "        valid_patients = patient_counts[patient_counts >= 3].index\n",
    "\n",
    "        growth_data = growth_data[growth_data[\"ID\"].isin(valid_patients)]\n",
    "\n",
    "        models = {}\n",
    "        smoothed_data = []\n",
    "\n",
    "        print(f\"Num Valid Patients: {len(valid_patients)}\")\n",
    "\n",
    "        for patient_id in valid_patients:\n",
    "            patient_data = growth_data[growth_data[\"ID\"] == patient_id].copy()            \n",
    "            patient_data = patient_data.sort_values(\"DayFromBirthNumeric\")\n",
    "\n",
    "            x = patient_data[\"DayFromBirthNumeric\"].values\n",
    "            y = patient_data[\"Value\"].values\n",
    "\n",
    "            if len(np.unique(x)) < 3:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                \"\"\"\n",
    "                This line currently uses past values, which introduces data leakage into\n",
    "                any predictor that uses the smoothed weight. Use a simple moving\n",
    "                average instead.\n",
    "                \"\"\"\n",
    "                fitted = lowess(y, x, frac=frac, return_sorted=False)\n",
    "\n",
    "                # fitted = pd.Series(y).expanding(min_periods=1).mean().values\n",
    "\n",
    "                interp_func = interp1d(\n",
    "                    x,\n",
    "                    fitted,\n",
    "                    kind=\"linear\",\n",
    "                    bounds_error=False,\n",
    "                    fill_value=\"extrapolate\",\n",
    "                )\n",
    "\n",
    "                models[patient_id] = interp_func\n",
    "\n",
    "                patient_data[\"fitted\"] = interp_func(x)\n",
    "                smoothed_data.append(patient_data)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error fitting LOESS for patient {patient_id}: {e}\")\n",
    "                continue\n",
    "\n",
    "        self.growth_models = models\n",
    "\n",
    "        smoothed_data = pd.concat(smoothed_data, axis=0).reset_index(drop=True)\n",
    "\n",
    "        return smoothed_data\n",
    "\n",
    "    def load_and_process_energy_data(self, energy_path: str) -> pd.DataFrame:\n",
    "        energy_data = pd.read_csv(energy_path)\n",
    "        energy_data[\"DateTime\"] = pd.to_datetime(energy_data[\"DateTime\"])\n",
    "        energy_data[\"StartDate\"] = energy_data[\"DateTime\"].dt.date\n",
    "\n",
    "        energy_data[\"Parenteral\"] = energy_data[\"TreatmentRoute\"].isin([\"IV\"])\n",
    "\n",
    "        daily_energy = (\n",
    "            energy_data.groupby([\"ID\", \"StartDate\", \"Parenteral\"])[\"Value\"]\n",
    "            .sum()\n",
    "            .reset_index()\n",
    "        )\n",
    "        daily_energy.columns = [\"ID\", \"StartDate\", \"Parenteral\", \"DailyEnergy\"]\n",
    "\n",
    "        daily_energy_pivot = daily_energy.pivot_table(\n",
    "            index=[\"ID\", \"StartDate\"],\n",
    "            columns=\"Parenteral\",\n",
    "            values=\"DailyEnergy\",\n",
    "            fill_value=0,\n",
    "        ).reset_index()\n",
    "\n",
    "        daily_energy_pivot.columns = [\n",
    "            \"ID\",\n",
    "            \"StartDate\",\n",
    "            \"DailyEnergyEnteral\",\n",
    "            \"DailyEnergyParenteral\",\n",
    "        ]\n",
    "        daily_energy_pivot[\"DailyEnergy\"] = (\n",
    "            daily_energy_pivot[\"DailyEnergyEnteral\"]\n",
    "            + daily_energy_pivot[\"DailyEnergyParenteral\"]\n",
    "        )\n",
    "\n",
    "        return daily_energy_pivot\n",
    "\n",
    "    def load_and_process_pulse_data(self, pulse_path):\n",
    "        pulse_data = pd.read_csv(pulse_path)\n",
    "        pulse_data[\"DateTime\"] = pd.to_datetime(pulse_data[\"DateTime\"])\n",
    "        pulse_data[\"StartDate\"] = pulse_data[\"DateTime\"].dt.date\n",
    "\n",
    "        daily_pulse = (\n",
    "            pulse_data.groupby([\"ID\", \"StartDate\"])[\"Value\"].mean().reset_index()\n",
    "        )\n",
    "        daily_pulse.columns = [\"ID\", \"StartDate\", \"MeanDailyPulse\"]\n",
    "\n",
    "        return daily_pulse\n",
    "\n",
    "    def load_and_process_spo2_data(self, spo2_path):\n",
    "        spo2_data = pd.read_csv(spo2_path)\n",
    "        spo2_data[\"DateTime\"] = pd.to_datetime(spo2_data[\"DateTime\"])\n",
    "        spo2_data[\"StartDate\"] = spo2_data[\"DateTime\"].dt.date\n",
    "\n",
    "        daily_spo2 = (\n",
    "            spo2_data.groupby([\"ID\", \"StartDate\"])[\"Value\"].mean().reset_index()\n",
    "        )\n",
    "        daily_spo2.columns = [\"ID\", \"StartDate\", \"MeanDailySPO2\"]\n",
    "\n",
    "        return daily_spo2\n",
    "\n",
    "    def load_and_process_stool_weight_data(self, stool_path):\n",
    "        stool_data = pd.read_csv(stool_path)\n",
    "        stool_data[\"DateTime\"] = pd.to_datetime(stool_data[\"DateTime\"])\n",
    "        stool_data[\"StartDate\"] = stool_data[\"DateTime\"].dt.date\n",
    "\n",
    "        daily_sum_stool_weight = (\n",
    "            stool_data.groupby([\"ID\", \"StartDate\"])[\"Value\"].sum().reset_index()\n",
    "        )\n",
    "        daily_sum_stool_weight.columns = [\"ID\", \"StartDate\", \"SumDailyStoolWeight\"]\n",
    "\n",
    "        return daily_sum_stool_weight\n",
    "\n",
    "    def load_and_process_bp_data(self, bp_path: str) -> pd.DataFrame:\n",
    "        bp_data = pd.read_csv(bp_path)\n",
    "        bp_data[\"DateTime\"] = pd.to_datetime(bp_data[\"DateTime\"])\n",
    "        bp_data[\"StartDate\"] = bp_data[\"DateTime\"].dt.date\n",
    "\n",
    "        bp_data[[\"systolic_pb\", \"diastolic_bp\"]] = (\n",
    "            bp_data[\"Value\"].str.split(\"/\", expand=True).astype(float)\n",
    "        )\n",
    "\n",
    "        daily_bp = (\n",
    "            bp_data.groupby([\"ID\", \"StartDate\"])[[\"systolic_pb\", \"diastolic_bp\"]]\n",
    "            .mean()\n",
    "            .reset_index()\n",
    "        )\n",
    "        daily_bp.columns = [\n",
    "            \"ID\",\n",
    "            \"StartDate\",\n",
    "            \"MeanDailySystolicBP\",\n",
    "            \"MeanDailyDiastolicBP\",\n",
    "        ]\n",
    "\n",
    "        return daily_bp\n",
    "\n",
    "    def load_and_process_resp_data(self, resp_path):\n",
    "        resp_data = pd.read_csv(resp_path)\n",
    "        resp_data[\"DateTime\"] = pd.to_datetime(resp_data[\"DateTime\"])\n",
    "        resp_data[\"StartDate\"] = resp_data[\"DateTime\"].dt.date\n",
    "\n",
    "        daily_resp = (\n",
    "            resp_data.groupby([\"ID\", \"StartDate\"])[\"Value\"].mean().reset_index()\n",
    "        )\n",
    "        daily_resp.columns = [\"ID\", \"StartDate\", \"MeanDailyResp\"]\n",
    "\n",
    "        return daily_resp\n",
    "\n",
    "    def load_and_process_io_data(self, io_path):\n",
    "        io_data = pd.read_csv(io_path)\n",
    "        io_data[\"StartDate\"] = pd.to_datetime(io_data[\"StartDate\"]).dt.date\n",
    "\n",
    "        io_pivot = io_data.pivot_table(\n",
    "            index=[\"ID\", \"StartDate\"], columns=\"OBX\", values=\"Value\", fill_value=0\n",
    "        ).reset_index()\n",
    "\n",
    "        return io_pivot\n",
    "\n",
    "    def predict_smoothed_weight(self, patient_id, day_from_birth):\n",
    "        if patient_id in self.growth_models:\n",
    "            return self.growth_models[patient_id](day_from_birth)\n",
    "        return np.nan\n",
    "\n",
    "    def calculate_weight_derivatives(self, patient_id, day_from_birth):\n",
    "        if patient_id not in self.growth_models:\n",
    "            return np.nan, np.nan\n",
    "\n",
    "        model = self.growth_models[patient_id]\n",
    "\n",
    "        try:\n",
    "            current_weight = model(day_from_birth)\n",
    "            next_day_weight = model(day_from_birth + 1)\n",
    "            daily_gain = next_day_weight - current_weight\n",
    "\n",
    "        except Exception:\n",
    "            daily_gain = np.nan\n",
    "\n",
    "        try:\n",
    "            week_later_weight = model(day_from_birth + 1)\n",
    "            week_earlier_weight = model(day_from_birth - 6)\n",
    "            weekly_gain = (week_later_weight - week_earlier_weight) / 7\n",
    "        except Exception:\n",
    "            weekly_gain = np.nan\n",
    "\n",
    "        return daily_gain, weekly_gain\n",
    "\n",
    "    def calculate_rolling_statistics(\n",
    "        self, df, patient_col, date_col, value_col, window=7\n",
    "    ):\n",
    "        df = df.sort_values([patient_col, date_col])\n",
    "\n",
    "        df[f\"{value_col}_rolling_mean\"] = df.groupby(patient_col)[value_col].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "        df[f\"{value_col}_rolling_std\"] = df.groupby(patient_col)[value_col].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).std()\n",
    "        )\n",
    "\n",
    "        return df\n",
    "\n",
    "    def consolidate_all_data(\n",
    "        self,\n",
    "        demographics,\n",
    "        energy_data,\n",
    "        pulse_data,\n",
    "        io_data,\n",
    "        spo2_data,\n",
    "        resp_data,\n",
    "        stool_data,\n",
    "        bp_data,\n",
    "    ):\n",
    "        consolidated = demographics[[\"ID\", \"DOB\", \"GA\", \"BW\", \"Sex\"]].copy()\n",
    "        consolidated = consolidated.merge(energy_data, on=\"ID\", how=\"inner\")\n",
    "        consolidated = consolidated.merge(\n",
    "            pulse_data, on=[\"ID\", \"StartDate\"], how=\"inner\"\n",
    "        )\n",
    "        consolidated = consolidated.merge(io_data, on=[\"ID\", \"StartDate\"], how=\"inner\")\n",
    "        consolidated = consolidated.merge(\n",
    "            spo2_data, on=[\"ID\", \"StartDate\"], how=\"inner\"\n",
    "        )\n",
    "        consolidated = consolidated.merge(\n",
    "            resp_data, on=[\"ID\", \"StartDate\"], how=\"inner\"\n",
    "        )\n",
    "        consolidated = consolidated.merge(\n",
    "            stool_data, on=[\"ID\", \"StartDate\"], how=\"inner\"\n",
    "        )\n",
    "        consolidated = consolidated.merge(bp_data, on=[\"ID\", \"StartDate\"], how=\"inner\")\n",
    "\n",
    "        consolidated[\"DayFromBirthNumeric\"] = (\n",
    "            pd.to_datetime(consolidated[\"StartDate\"]) - consolidated[\"DOB\"]\n",
    "        ).dt.days.astype(float)\n",
    "\n",
    "        consolidated[\"PMADays\"] = (\n",
    "            7 * consolidated[\"GA\"] + consolidated[\"DayFromBirthNumeric\"]\n",
    "        ).astype(int)\n",
    "\n",
    "        consolidated[\"SexMInt\"] = (consolidated[\"Sex\"] == \"M\").astype(int)\n",
    "\n",
    "        consolidated[\"SmoothedWeightG\"] = consolidated.apply(\n",
    "            lambda row: self.predict_smoothed_weight(\n",
    "                row[\"ID\"], row[\"DayFromBirthNumeric\"]\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        weight_gains = consolidated.apply(\n",
    "            lambda row: self.calculate_weight_derivatives(\n",
    "                row[\"ID\"], row[\"DayFromBirthNumeric\"]\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        consolidated[\"DailyWeightGainGPerDay\"] = [x[0] for x in weight_gains]\n",
    "        consolidated[\"WeeklyWeightGainGPerDay\"] = [x[1] for x in weight_gains]\n",
    "\n",
    "        consolidated[\"DailyEnergyPerKG\"] = (\n",
    "            consolidated[\"DailyEnergy\"] * 1000 / consolidated[\"SmoothedWeightG\"]\n",
    "        )\n",
    "        consolidated[\"DailyEnergyParenteralPerKG\"] = (\n",
    "            consolidated[\"DailyEnergyParenteral\"]\n",
    "            * 1000\n",
    "            / consolidated[\"SmoothedWeightG\"]\n",
    "        )\n",
    "        consolidated[\"DailyWeightGainGPerKGPerDay\"] = (\n",
    "            consolidated[\"DailyWeightGainGPerDay\"]\n",
    "            * 1000\n",
    "            / consolidated[\"SmoothedWeightG\"]\n",
    "        )\n",
    "        consolidated[\"WeeklyWeightGainGPerKGPerDay\"] = (\n",
    "            consolidated[\"WeeklyWeightGainGPerDay\"]\n",
    "            * 1000\n",
    "            / consolidated[\"SmoothedWeightG\"]\n",
    "        )\n",
    "        consolidated[\"MeanDailyPulsePerKG\"] = (\n",
    "            consolidated[\"MeanDailyPulse\"] * 1000 / consolidated[\"SmoothedWeightG\"]\n",
    "        )\n",
    "\n",
    "        if \"DailyFluidIntake\" in consolidated.columns:\n",
    "            consolidated[\"DailyFluidIntakePerKG\"] = (\n",
    "                consolidated[\"DailyFluidIntake\"]\n",
    "                * 1000\n",
    "                / consolidated[\"SmoothedWeightG\"]\n",
    "            )\n",
    "        if \"DailyFluidOutput\" in consolidated.columns:\n",
    "            consolidated[\"DailyFluidOutputPerKG\"] = (\n",
    "                consolidated[\"DailyFluidOutput\"]\n",
    "                * 1000\n",
    "                / consolidated[\"SmoothedWeightG\"]\n",
    "            )\n",
    "\n",
    "        if \"DailyFluidIntake\" in consolidated.columns:\n",
    "            consolidated = self.calculate_rolling_statistics(\n",
    "                consolidated, \"ID\", \"StartDate\", \"DailyFluidIntake\"\n",
    "            )\n",
    "        if \"DailyFluidOutput\" in consolidated.columns:\n",
    "            consolidated = self.calculate_rolling_statistics(\n",
    "                consolidated, \"ID\", \"StartDate\", \"DailyFluidOutput\"\n",
    "            )\n",
    "\n",
    "        consolidated[\"GrowthStage\"] = np.where(\n",
    "            consolidated[\"PMADays\"] < 184,\n",
    "            1,\n",
    "            np.where(consolidated[\"PMADays\"] < 237, 2, 3),\n",
    "        )\n",
    "\n",
    "        consolidated = consolidated[consolidated[\"PMADays\"] < 280]\n",
    "\n",
    "        patient_ids = consolidated[\"ID\"].unique()\n",
    "        train_ids = np.random.choice(\n",
    "            patient_ids, size=int(len(patient_ids) * 0.5), replace=False\n",
    "        )\n",
    "        consolidated[\"Train\"] = consolidated[\"ID\"].isin(train_ids)\n",
    "\n",
    "        self.consolidated_data = consolidated\n",
    "        return consolidated\n",
    "\n",
    "    def get_normal_weekly_growth(self, ga, week_from_birth, weight_g):\n",
    "        pma = ga + week_from_birth - 1\n",
    "\n",
    "        if week_from_birth == 1:\n",
    "            return 0\n",
    "        elif pma > 34:\n",
    "            return 17 * weight_g / 1000\n",
    "        else:\n",
    "            return 30\n",
    "\n",
    "    def add_normal_growth_comparison(self):\n",
    "        if self.consolidated_data is None:\n",
    "            raise ValueError(\"No consolidated data available.\")\n",
    "\n",
    "        self.consolidated_data[\"WeekFromBirth\"] = (\n",
    "            self.consolidated_data[\"DayFromBirthNumeric\"] / 7\n",
    "        ).astype(int) + 1\n",
    "\n",
    "        self.consolidated_data[\"NormalWeeklyGrowthGPerDay\"] = (\n",
    "            self.consolidated_data.apply(\n",
    "                lambda row: self.get_normal_weekly_growth(\n",
    "                    row[\"GA\"], row[\"WeekFromBirth\"], row[\"SmoothedWeightG\"]\n",
    "                ),\n",
    "                axis=1,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return self.consolidated_data\n",
    "\n",
    "    def create_feature_list(self):\n",
    "        base_features = [\n",
    "            \"GA\",\n",
    "            \"BW\",\n",
    "            \"DailyEnergy\",\n",
    "            \"DailyEnergyParenteral\",\n",
    "            \"DayFromBirthNumeric\",\n",
    "            \"SmoothedWeightG\",\n",
    "            \"MeanDailyPulse\",\n",
    "            \"PMADays\",\n",
    "            \"SexMInt\",\n",
    "            \"DailyEnergyPerKG\",\n",
    "            \"DailyEnergyParenteralPerKG\",\n",
    "            \"MeanDailyPulsePerKG\",\n",
    "            \"MeanDailySystolicBP\",\n",
    "            \"MeanDailyDiastolicBP\",\n",
    "            \"SumDailyStoolWeight\",\n",
    "            \"MeanDailyResp\",\n",
    "            \"MeanDailySPO2\",\n",
    "        ]\n",
    "\n",
    "        if \"DailyFluidIntake\" in self.consolidated_data.columns:\n",
    "            base_features.extend(\n",
    "                [\n",
    "                    \"DailyFluidIntake\",\n",
    "                    \"DailyFluidIntakePerKG\",\n",
    "                    \"DailyFluidIntake_rolling_mean\",\n",
    "                    \"DailyFluidIntake_rolling_std\",\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        if \"DailyFluidOutput\" in self.consolidated_data.columns:\n",
    "            base_features.extend(\n",
    "                [\n",
    "                    \"DailyFluidOutput\",\n",
    "                    \"DailyFluidOutputPerKG\",\n",
    "                    \"DailyFluidOutput_rolling_mean\",\n",
    "                    \"DailyFluidOutput_rolling_std\",\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        extended_features = base_features.copy()\n",
    "        for feature in base_features:\n",
    "            if feature in self.consolidated_data.columns:\n",
    "                extended_features.append(f\"{feature}_squared\")\n",
    "                extended_features.append(f\"{feature}_sqrt\")\n",
    "\n",
    "        return extended_features\n",
    "\n",
    "    def create_polynomial_features(self):\n",
    "        if self.consolidated_data is None:\n",
    "            raise ValueError(\"No consolidated data available.\")\n",
    "\n",
    "        base_features = self.create_feature_list()\n",
    "\n",
    "        for feature in base_features:\n",
    "            if feature in self.consolidated_data.columns:\n",
    "                self.consolidated_data[f\"{feature}_squared\"] = (\n",
    "                    self.consolidated_data[feature] ** 2\n",
    "                )\n",
    "\n",
    "                self.consolidated_data[f\"{feature}_sqrt\"] = np.where(\n",
    "                    self.consolidated_data[feature] >= 0,\n",
    "                    self.consolidated_data[feature] ** 0.5,\n",
    "                    np.nan,\n",
    "                )\n",
    "\n",
    "        return self.consolidated_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97af30b2",
   "metadata": {},
   "source": [
    "This section of code is simply responsible for calling all of the top level functions found in the `GrowthDataTransformation()` class. The final consolidated csv file is built and saved here. Note that if the consolidated csv is already created, this data creation code blocks do not need to be run again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58886ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = GrowthDataTransformation()\n",
    "\n",
    "demographics = analysis.load_and_process_demographics(\n",
    "    os.path.join(DATA_ROOT, \"Demographics.csv\")\n",
    ")\n",
    "weight_data = analysis.load_and_process_weight_data(\n",
    "    os.path.join(DATA_ROOT, \"WeightObservations.csv\")\n",
    ")\n",
    "\n",
    "growth_data = analysis.create_growth_tibble(demographics, weight_data)\n",
    "smoothed_growth = analysis.fit_growth_models(growth_data)\n",
    "\n",
    "energy_data = analysis.load_and_process_energy_data(\n",
    "    os.path.join(DATA_ROOT, \"CalculatedEnergyObservations.csv\")\n",
    ")\n",
    "\n",
    "pulse_data = analysis.load_and_process_pulse_data(\n",
    "    os.path.join(DATA_ROOT, \"PulseObservations.csv\")\n",
    ")\n",
    "\n",
    "spo2_data = analysis.load_and_process_spo2_data(os.path.join(DATA_ROOT, \"SPO2Obs.csv\"))\n",
    "\n",
    "resp_data = analysis.load_and_process_resp_data(os.path.join(DATA_ROOT, \"RespObs.csv\"))\n",
    "\n",
    "\n",
    "io_data = analysis.load_and_process_io_data(os.path.join(DATA_ROOT, \"DailyIO.csv\"))\n",
    "\n",
    "stool_data = analysis.load_and_process_stool_weight_data(\n",
    "    os.path.join(DATA_ROOT, \"StoolWeightObservations.csv\")\n",
    ")\n",
    "\n",
    "bp_data = analysis.load_and_process_bp_data(\n",
    "    os.path.join(DATA_ROOT, \"BPObservations.csv\")\n",
    ")\n",
    "\n",
    "consolidated = analysis.consolidate_all_data(\n",
    "    demographics,\n",
    "    energy_data,\n",
    "    pulse_data,\n",
    "    io_data,\n",
    "    spo2_data,\n",
    "    resp_data,\n",
    "    stool_data,\n",
    "    bp_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9c8f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = analysis.add_normal_growth_comparison()\n",
    "final_data = analysis.create_polynomial_features()\n",
    "\n",
    "final_data.to_csv(os.path.join(DATA_ROOT, \"Consolidated.csv\"), index=False)\n",
    "\n",
    "final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669c474a",
   "metadata": {},
   "source": [
    "## Train and Test Data Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37e6e83",
   "metadata": {},
   "source": [
    "Loading the consolidated csv file and dropping any missing values in `WeeklyWeightGainGPerKGPerDay`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a8c4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_ROOT, \"Consolidated.csv\"))\n",
    "\n",
    "df = df.dropna(subset=[\"WeeklyWeightGainGPerKGPerDay\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceb2646",
   "metadata": {},
   "source": [
    "1. We drop all values that are derivatives of `WeeklyWeightGainGPerKGPerDay` to avoid data leakage. Data is then split into X and y.\n",
    "2. For each categorical/text column:\n",
    "    - Missing values are filled with \"Missing\" (as a string label).\n",
    "    - Each unique value is replaced with an integer (e.g., \"A\", \"B\", \"Missing\" → 0, 1, 2).\n",
    "3. For each numeric feature, any missing values are filled in with the median value of that column.\n",
    "4. All values of X are limited to between $-1000000$ and $1000000$. This is to stop massive values from overflowing.\n",
    "5. Splits the data into 80% train, 20% test. A seed is set to ensure the split stays constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12f8fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\n",
    "    columns=[\n",
    "        \"WeeklyWeightGainGPerDay\",\n",
    "        \"WeeklyWeightGainGPerKGPerDay\",\n",
    "        \"DailyWeightGainGPerDay\",\n",
    "        \"DailyWeightGainGPerKGPerDay\",\n",
    "    ]\n",
    ")\n",
    "y = df[\"WeeklyWeightGainGPerKGPerDay\"]\n",
    "\n",
    "for col in X.select_dtypes(include=[\"object\", \"category\"]).columns:\n",
    "    X[col] = X[col].fillna(\"Missing\")\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "\n",
    "for col in X.select_dtypes(include=[\"float64\", \"int64\"]).columns:\n",
    "    X[col] = X[col].fillna(X[col].median())\n",
    "\n",
    "X = X.clip(lower=-1e6, upper=1e6)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d9bd58",
   "metadata": {},
   "source": [
    "A helper function that returns Mean Absolute Error, Mean Squared Error, and $R^2$ value for a given prediction and ground truth vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c307e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_results(preds, y_test):\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    mse = mean_squared_error(y_test, preds)\n",
    "    r2 = r2_score(y_test, preds)\n",
    "\n",
    "    return f\"MAE: {mae} | MSE: {mse} | r^2: {r2}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f91d7d1",
   "metadata": {},
   "source": [
    "An xgboost regressor model is trained using a random search with 3 fold cross validation. Note that the commented out blcok uses a Grid Search instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d135898",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid: dict[str, Union[list[int], list[float]]] = {\n",
    "    \"n_estimators\": [100, 300, 500],\n",
    "    \"max_depth\": [3, 6, 9],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.3],\n",
    "    \"subsample\": [0.7, 0.9, 1.0],\n",
    "    \"colsample_bytree\": [0.7, 0.9, 1.0],\n",
    "    \"gamma\": [0, 1, 5],\n",
    "}\n",
    "\n",
    "model = xg.XGBRegressor(objective=\"reg:squarederror\", seed=42)\n",
    "\"\"\"\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    verbose=2,\n",
    "    n_jobs=1,\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best CV score (negative MSE):\", grid_search.best_score_)\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print(model_results(preds=y_pred, y_test=y_test))\n",
    "\"\"\"\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=128,\n",
    "    cv=3,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    verbose=2,\n",
    "    n_jobs=1,\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "print(\"Best CV score (negative MSE):\", random_search.best_score_)\n",
    "\n",
    "y_pred = random_search.predict(X_test)\n",
    "print(model_results(preds=y_pred, y_test=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed502b2d",
   "metadata": {},
   "source": [
    "Listing feature importances for the trained random search model. This is used to ID features with the strongest impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeb9867",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.Series(\n",
    "    random_search.best_estimator_.feature_importances_, index=X_train.columns\n",
    ").sort_values(ascending=False)\n",
    "importances.head(29)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e75f58b",
   "metadata": {},
   "source": [
    "I tried to replicate the graph shown from your presentation, and as shown below, I was unsuccessful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ececee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = X_test.copy()\n",
    "df[\"WeeklyWeightGainPerKgPerDay\"] = y_test\n",
    "df[\"GA * 7 + DayFromBirthNumeric\"] = df[\"GA\"] * 7 + df[\"DayFromBirthNumeric\"]\n",
    "\n",
    "n_patients = 128\n",
    "patient_ids = df[\"ID\"].unique()[:n_patients]\n",
    "df_subset = df[df[\"ID\"].isin(patient_ids)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "color_cycle = plt.cm.tab20(np.linspace(0, 1, n_patients))\n",
    "patient_colors = {pid: color_cycle[i] for i, pid in enumerate(patient_ids)}\n",
    "\n",
    "\n",
    "for patient_id, group in df_subset.groupby(\"ID\"):\n",
    "    color = patient_colors[patient_id]\n",
    "\n",
    "    x_vals = group[\"GA * 7 + DayFromBirthNumeric\"].values\n",
    "    y_real = group[\"WeeklyWeightGainPerKgPerDay\"].values\n",
    "\n",
    "    model_X = X_test.loc[group.index]\n",
    "    y_pred = random_search.predict(model_X)\n",
    "\n",
    "    ax.scatter(x_vals, y_real, s=22, alpha=0.7, color=color)\n",
    "\n",
    "    for i in range(len(x_vals)):\n",
    "        x_head = x_vals[i]\n",
    "        y_head = y_real[i]\n",
    "        pred_gain = y_pred[i]\n",
    "\n",
    "        x_tail = x_head + 7\n",
    "        y_tail = y_head + pred_gain * 7\n",
    "\n",
    "        ax.plot([x_head, x_tail], [y_head, y_tail], color=color, linewidth=1.7)\n",
    "\n",
    "ax.set_xlabel(\"GA * 7 + DayFromBirthNumeric\", fontsize=14)\n",
    "ax.set_ylabel(\"WeeklyWeightGainGPerKGPerDay\", fontsize=14)\n",
    "ax.set_title(\n",
    "    f\"First {n_patients} patients: real values (dots) & model weekly vector (lines)\",\n",
    "    fontsize=15,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2991cd",
   "metadata": {},
   "source": [
    "## Training using Optuna\n",
    "This code block uses Optuna, an optimization library to train the XGB model. The model is tested 1000 times with different hyperparameters, and the best model is chosen at the end. The process can be stopped at any time, and the best model will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00efd1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_func(trial: optuna.Trial):\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 1000)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 10, 50)\n",
    "    min_child_weight = trial.suggest_int(\"min_child_weight\", 1, 32)\n",
    "    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0, 1)\n",
    "    objective = trial.suggest_categorical(\n",
    "        \"objective\",\n",
    "        [\n",
    "            \"reg:squarederror\",\n",
    "            \"reg:absoluteerror\",\n",
    "        ],\n",
    "    )\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.005, 0.25)\n",
    "\n",
    "    model = xg.XGBRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_child_weight=min_child_weight,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        objective=objective,\n",
    "        learning_rate=learning_rate,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    score = r2_score(y_test, y_pred)\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\", sampler=optuna.samplers.RandomSampler(seed=42)\n",
    ")\n",
    "study.optimize(objective_func, n_trials=1000)\n",
    "\n",
    "print(\"Best Trial\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(f\"Value: {trial.value}\")\n",
    "\n",
    "print(\"Params:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297519a9",
   "metadata": {},
   "source": [
    "This block prints the statistics for the best model found using Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51826f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "\n",
    "best_n_estimators = best_params[\"n_estimators\"]\n",
    "best_max_depth = best_params[\"max_depth\"]\n",
    "best_min_child_weight = best_params[\"min_child_weight\"]\n",
    "best_colsample_bytree = best_params[\"colsample_bytree\"]\n",
    "best_objective = best_params[\"objective\"]\n",
    "best_learning_rate = best_params[\"learning_rate\"]\n",
    "\n",
    "best_model = xg.XGBRegressor(\n",
    "    n_estimators=best_n_estimators,\n",
    "    max_depth=best_max_depth,\n",
    "    min_child_weight=best_min_child_weight,\n",
    "    colsample_bytree=best_colsample_bytree,\n",
    "    objective=best_objective,\n",
    "    learning_rate=best_learning_rate,\n",
    ")\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_xgb_optuna_200_trials = best_model.predict(X_test)\n",
    "\n",
    "model_results(y_pred_xgb_optuna_200_trials, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpsc (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
