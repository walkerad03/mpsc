{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abf53261",
   "metadata": {},
   "source": [
    "# Full Project Code\n",
    "\n",
    "The first part of this notebook is responsible for transforming the raw json data into a usable tabular format. Due to the robustness of XGBoost as an algorithm, we do not need to handle missing values, and can pass them directly as nulls.\n",
    "\n",
    "Note that we must have created the relevant csv files before running this code, which can be done using `scripts/convert_json_data.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308c59e5",
   "metadata": {},
   "source": [
    "## Setup\n",
    "This part of the notebook imports all dependencies, and sets up any global constants we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79552a0a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import xgboost as xg\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f6248e5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(\"/\", \"home\", \"walkerdavis\", \"projects\", \"mpsc\"))\n",
    "DATA_ROOT = os.path.join(\"data\", \"processed\")\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73a43b3",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "The `GrowthDataTransformation` class encapsulates all data loading and feature engineering for the model.\n",
    "\n",
    "### `__init__(self)`\n",
    "Initializes the class instance. Sets up placeholders for the consolidated output dataframe (`self.consolidated_data`) and a dictionary for patient-specific growth model interpolators (`self.growth_models`).\n",
    "\n",
    "### `load_and_process_demographics(self, demographics_path: str)`\n",
    "Reads the demographics CSV and cleans the resulting dataframe:\n",
    "- Converts Gestational Age from a string to numeric, and extracts the integer.\n",
    "- Parses DOB, and Data and Fluid Start and End as datetime objects.\n",
    "- Filters out records with future fluid dates, missing/invalid GA, or birth weight outliers.\n",
    "\n",
    "### `load_and_process_weight_data(self, weight_path: str):`\n",
    "Loads and cleans weight measurement data.\n",
    "- Converts weights to grams (assumes original is in ounces).\n",
    "- Adds a column marking these as weight measurements (`OBX`).\n",
    "- Converts timestamps to datetime.\n",
    "- Removes rows with missing weight values.\n",
    "\n",
    "### `create_growth_tibble(self, demographics, weight_data)`:\n",
    "Joins demographics and weight data, and computes time-relative variables.\n",
    "- Merges demographics and weight by `ID`.\n",
    "- Calculates days since birth for each weight entry.\n",
    "- Computes post-menstrual age (PMA) in days and weeks.\n",
    "\n",
    "### `fit_growth_models(self, growth_data, frac=0.3)`:\n",
    "Fits a smoothed weight curve for each patient and saves interpolation functions to the main class.\n",
    "- Filters patients with at least 3 weight observations.\n",
    "- For each, sorts by age, fits a LOESS smoothed curve to weight over time. NOTE: I had attemtped to use an alternative smoothing algorithm that did not need to reference future values, however I ran into issue where the algorithm would generate massive amounts of missing values later in the dataset.\n",
    "- Builds an interpolating function for each patient’s smoothed weights.\n",
    "- Stores models and returns all patient data with smoothed weights included.\n",
    "\n",
    "### `load_and_process_energy_data(self, energy_path: str)`:\n",
    "Loads and processes daily energy intake data.\n",
    "- Reads CSV, converts timestamps.\n",
    "- Marks whether energy was delivered parenterally (IV) or enterally.\n",
    "- Aggregates total energy per day and route.\n",
    "- Pivots into one row per patient per day, with enteral, parenteral, and total energy columns.\n",
    "\n",
    "### `load_and_process_pulse_data(self, pulse_path)`:\n",
    "Loads and processes pulse rate data.\n",
    "- Reads CSV, extracts date.\n",
    "- Aggregates mean daily pulse for each patient.\n",
    "\n",
    "### `load_and_process_spo2_data(self, spo2_path)`:\n",
    "Loads and processes daily oxygen saturation (SpO2) data.\n",
    "- Reads CSV, extracts date.\n",
    "- Aggregates mean daily SpO2 for each patient.\n",
    "\n",
    "### `load_and_process_stool_weight_data(self, stool_path)`:\n",
    "Loads and sums daily stool output.\n",
    "- Reads CSV, extracts date.\n",
    "- Aggregates total stool output per patient per day.\n",
    "\n",
    "### `load_and_process_bp_data(self, bp_path: str)`:\n",
    "Loads and processes blood pressure readings.\n",
    "- Reads CSV, parses dates.\n",
    "- Splits BP readings into systolic and diastolic values.\n",
    "- Aggregates mean daily systolic/diastolic BP per patient.\n",
    "\n",
    "### `load_and_process_resp_data(self, resp_path)`:\n",
    "Loads and processes daily respiratory rate data.\n",
    "- Reads CSV, extracts date.\n",
    "- Aggregates mean daily respiratory rate per patient.\n",
    "\n",
    "### `load_and_process_io_data(self, io_path)`:\n",
    "Loads and processes daily input/output data.\n",
    "- Reads CSV, parses date.\n",
    "- Pivots to one row per patient per day, with each OBX variable as a column.\n",
    "\n",
    "### `predict_smoothed_weight(self, patient_id, day_from_birth)`:\n",
    "For a given patient and day, returns the smoothed weight value using their fitted curve.\n",
    "- Looks up the interpolation model and evaluates at the requested day.\n",
    "\n",
    "### `calculate_weight_derivatives(self, patient_id, day_from_birth)`:\n",
    "Calculates the patient’s weight velocity (gain) on a specific day.\n",
    "- Uses smoothed weights to estimate daily and weekly gains via finite differences.\n",
    "\n",
    "### `calculate_rolling_statistics(self, df, patient_col, date_col, value_col, window=7)`:\n",
    "Adds rolling mean and standard deviation for a feature, per patient.\n",
    "- Sorts and groups by patient, computes rolling window stats for a given column.\n",
    "\n",
    "### `consolidate_all_data(self, ...)`:\n",
    "Merges all available data into a single, comprehensive table and computes derived features.\n",
    "- Joins all processed daily data on patient and date.\n",
    "- Computes key clinical time variables (days since birth, PMA).\n",
    "- Encodes sex.\n",
    "- Fills in smoothed weights and weight gains.\n",
    "- Calculates per-kg rates for energy, fluid, and other features.\n",
    "- Adds rolling stats and a categorical growth stage.\n",
    "- Filters to relevant PMA window.\n",
    "- Randomly splits patients into train/test groups.\n",
    "\n",
    "### `get_normal_weekly_growth(self, ga, week_from_birth, weight_g)`:\n",
    "Returns the expected (“normal”) weekly weight gain for a given gestational age and week.\n",
    "- Based on clinical conventions, uses fixed or proportional values by age.\n",
    "\n",
    "### `add_normal_growth_comparison(self)`:\n",
    "Appends a column for expected normal weekly growth to the consolidated data.\n",
    "- For each row, computes the expected growth using get_normal_weekly_growth.\n",
    "\n",
    "### `create_feature_list(self)`:\n",
    "Assemble a full feature list\n",
    "- Lists key base features (per-kg, etc.).\n",
    "- Optionally includes fluid intake/output and their rolling stats, if the base features are present.\n",
    "- Extends list to include squared and square root transformations.\n",
    "\n",
    "### `create_polynomial_features(self)`:\n",
    "Adds polynomial and root-transformed versions of all base features.\n",
    "- For each feature, adds a squared and square root column (if non-negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e164f6c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class GrowthDataTransformation:\n",
    "    def __init__(self):\n",
    "        self.consolidated_data = None\n",
    "        self.growth_models = {}\n",
    "\n",
    "    def load_and_process_demographics(self, demographics_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Load and process demographics data\"\"\"\n",
    "        demographics = pd.read_csv(demographics_path)\n",
    "\n",
    "        demographics[\"GA\"] = pd.to_numeric(\n",
    "            demographics[\"GA\"].str.extract(r\"(\\d+)\")[0], errors=\"coerce\"\n",
    "        )\n",
    "\n",
    "        demographics[\"DOB\"] = pd.to_datetime(demographics[\"DOB\"])\n",
    "        demographics[\"DataStart\"] = pd.to_datetime(demographics[\"DataStart\"])\n",
    "        demographics[\"DataEnd\"] = pd.to_datetime(demographics[\"DataEnd\"])\n",
    "        demographics[\"FluidStart\"] = pd.to_datetime(demographics[\"FluidStart\"])\n",
    "        demographics[\"FluidEnd\"] = pd.to_datetime(demographics[\"FluidEnd\"])\n",
    "\n",
    "        demographics = demographics[\n",
    "            (demographics[\"FluidStart\"] < pd.to_datetime(\"2200-01-01\"))\n",
    "            & (demographics[\"GA\"].notna())\n",
    "            & (demographics[\"GA\"] > 20)\n",
    "            & (demographics[\"BW\"].notna())\n",
    "            & (demographics[\"BW\"] > 200)\n",
    "        ]\n",
    "\n",
    "        return demographics\n",
    "\n",
    "    def load_and_process_weight_data(self, weight_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Load and process weight data\"\"\"\n",
    "        weight_data = pd.read_csv(weight_path)\n",
    "        assert isinstance(weight_data, pd.DataFrame)\n",
    "\n",
    "        weight_data[\"Value\"] = weight_data[\"Value\"] * 28.35\n",
    "        weight_data[\"OBX\"] = \"WeightG\"\n",
    "        weight_data[\"DateTime\"] = pd.to_datetime(weight_data[\"DateTime\"])\n",
    "\n",
    "        weight_data = weight_data.dropna(subset=[\"Value\"])\n",
    "\n",
    "        return weight_data[[\"ID\", \"DateTime\", \"OBX\", \"Value\"]]\n",
    "\n",
    "    def create_growth_tibble(\n",
    "        self, demographics: pd.DataFrame, weight_data: pd.DataFrame\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Join demographics and weight data: compute additional variables\"\"\"\n",
    "        growth_data = demographics[[\"ID\", \"DOB\", \"GA\", \"BW\", \"Sex\"]].merge(\n",
    "            weight_data, on=\"ID\", how=\"inner\"\n",
    "        )\n",
    "\n",
    "        growth_data[\"DayFromBirthNumeric\"] = (\n",
    "            growth_data[\"DateTime\"] - growth_data[\"DOB\"]\n",
    "        ).dt.days.astype(float)\n",
    "\n",
    "        growth_data[\"PMADays\"] = (\n",
    "            7 * growth_data[\"GA\"] + growth_data[\"DayFromBirthNumeric\"]\n",
    "        ).astype(int)\n",
    "\n",
    "        growth_data[\"PMAWeeks\"] = (growth_data[\"PMADays\"] / 7).astype(int)\n",
    "\n",
    "        return growth_data\n",
    "\n",
    "    def fit_growth_models(self, growth_data: pd.DataFrame, frac: float = 0.3):\n",
    "        patient_counts = growth_data.groupby(\"ID\").size()\n",
    "        valid_patients = patient_counts[patient_counts >= 3].index\n",
    "\n",
    "        growth_data = growth_data[growth_data[\"ID\"].isin(valid_patients)]\n",
    "\n",
    "        models = {}\n",
    "        smoothed_data = []\n",
    "\n",
    "        print(f\"Num Valid Patients: {len(valid_patients)}\")\n",
    "\n",
    "        for patient_id in valid_patients:\n",
    "            patient_data = growth_data[growth_data[\"ID\"] == patient_id].copy()            \n",
    "            patient_data = patient_data.sort_values(\"DayFromBirthNumeric\")\n",
    "\n",
    "            x = patient_data[\"DayFromBirthNumeric\"].values\n",
    "            y = patient_data[\"Value\"].values\n",
    "\n",
    "            if len(np.unique(x)) < 3:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                \"\"\"\n",
    "                This line currently uses past values, which introduces data leakage into\n",
    "                any predictor that uses the smoothed weight. Use a simple moving\n",
    "                average instead.\n",
    "                \"\"\"\n",
    "                fitted = lowess(y, x, frac=frac, return_sorted=False)\n",
    "\n",
    "                # fitted = pd.Series(y).expanding(min_periods=1).mean().values\n",
    "\n",
    "                interp_func = interp1d(\n",
    "                    x,\n",
    "                    fitted,\n",
    "                    kind=\"linear\",\n",
    "                    bounds_error=False,\n",
    "                    fill_value=\"extrapolate\",\n",
    "                )\n",
    "\n",
    "                models[patient_id] = interp_func\n",
    "\n",
    "                patient_data[\"fitted\"] = interp_func(x)\n",
    "                smoothed_data.append(patient_data)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error fitting LOESS for patient {patient_id}: {e}\")\n",
    "                continue\n",
    "\n",
    "        self.growth_models = models\n",
    "\n",
    "        smoothed_data = pd.concat(smoothed_data, axis=0).reset_index(drop=True)\n",
    "\n",
    "        return smoothed_data\n",
    "\n",
    "    def load_and_process_energy_data(self, energy_path: str) -> pd.DataFrame:\n",
    "        energy_data = pd.read_csv(energy_path)\n",
    "        energy_data[\"DateTime\"] = pd.to_datetime(energy_data[\"DateTime\"])\n",
    "        energy_data[\"StartDate\"] = energy_data[\"DateTime\"].dt.date\n",
    "\n",
    "        energy_data[\"Parenteral\"] = energy_data[\"TreatmentRoute\"].isin([\"IV\"])\n",
    "\n",
    "        daily_energy = (\n",
    "            energy_data.groupby([\"ID\", \"StartDate\", \"Parenteral\"])[\"Value\"]\n",
    "            .sum()\n",
    "            .reset_index()\n",
    "        )\n",
    "        daily_energy.columns = [\"ID\", \"StartDate\", \"Parenteral\", \"DailyEnergy\"]\n",
    "\n",
    "        daily_energy_pivot = daily_energy.pivot_table(\n",
    "            index=[\"ID\", \"StartDate\"],\n",
    "            columns=\"Parenteral\",\n",
    "            values=\"DailyEnergy\",\n",
    "            fill_value=0,\n",
    "        ).reset_index()\n",
    "\n",
    "        daily_energy_pivot.columns = [\n",
    "            \"ID\",\n",
    "            \"StartDate\",\n",
    "            \"DailyEnergyEnteral\",\n",
    "            \"DailyEnergyParenteral\",\n",
    "        ]\n",
    "        daily_energy_pivot[\"DailyEnergy\"] = (\n",
    "            daily_energy_pivot[\"DailyEnergyEnteral\"]\n",
    "            + daily_energy_pivot[\"DailyEnergyParenteral\"]\n",
    "        )\n",
    "\n",
    "        return daily_energy_pivot\n",
    "\n",
    "    def load_and_process_pulse_data(self, pulse_path):\n",
    "        pulse_data = pd.read_csv(pulse_path)\n",
    "        pulse_data[\"DateTime\"] = pd.to_datetime(pulse_data[\"DateTime\"])\n",
    "        pulse_data[\"StartDate\"] = pulse_data[\"DateTime\"].dt.date\n",
    "\n",
    "        daily_pulse = (\n",
    "            pulse_data.groupby([\"ID\", \"StartDate\"])[\"Value\"].mean().reset_index()\n",
    "        )\n",
    "        daily_pulse.columns = [\"ID\", \"StartDate\", \"MeanDailyPulse\"]\n",
    "\n",
    "        return daily_pulse\n",
    "\n",
    "    def load_and_process_spo2_data(self, spo2_path):\n",
    "        spo2_data = pd.read_csv(spo2_path)\n",
    "        spo2_data[\"DateTime\"] = pd.to_datetime(spo2_data[\"DateTime\"])\n",
    "        spo2_data[\"StartDate\"] = spo2_data[\"DateTime\"].dt.date\n",
    "\n",
    "        daily_spo2 = (\n",
    "            spo2_data.groupby([\"ID\", \"StartDate\"])[\"Value\"].mean().reset_index()\n",
    "        )\n",
    "        daily_spo2.columns = [\"ID\", \"StartDate\", \"MeanDailySPO2\"]\n",
    "\n",
    "        return daily_spo2\n",
    "\n",
    "    def load_and_process_stool_weight_data(self, stool_path):\n",
    "        stool_data = pd.read_csv(stool_path)\n",
    "        stool_data[\"DateTime\"] = pd.to_datetime(stool_data[\"DateTime\"])\n",
    "        stool_data[\"StartDate\"] = stool_data[\"DateTime\"].dt.date\n",
    "\n",
    "        daily_sum_stool_weight = (\n",
    "            stool_data.groupby([\"ID\", \"StartDate\"])[\"Value\"].sum().reset_index()\n",
    "        )\n",
    "        daily_sum_stool_weight.columns = [\"ID\", \"StartDate\", \"SumDailyStoolWeight\"]\n",
    "\n",
    "        return daily_sum_stool_weight\n",
    "\n",
    "    def load_and_process_bp_data(self, bp_path: str) -> pd.DataFrame:\n",
    "        bp_data = pd.read_csv(bp_path)\n",
    "        bp_data[\"DateTime\"] = pd.to_datetime(bp_data[\"DateTime\"])\n",
    "        bp_data[\"StartDate\"] = bp_data[\"DateTime\"].dt.date\n",
    "\n",
    "        bp_data[[\"systolic_pb\", \"diastolic_bp\"]] = (\n",
    "            bp_data[\"Value\"].str.split(\"/\", expand=True).astype(float)\n",
    "        )\n",
    "\n",
    "        daily_bp = (\n",
    "            bp_data.groupby([\"ID\", \"StartDate\"])[[\"systolic_pb\", \"diastolic_bp\"]]\n",
    "            .mean()\n",
    "            .reset_index()\n",
    "        )\n",
    "        daily_bp.columns = [\n",
    "            \"ID\",\n",
    "            \"StartDate\",\n",
    "            \"MeanDailySystolicBP\",\n",
    "            \"MeanDailyDiastolicBP\",\n",
    "        ]\n",
    "\n",
    "        return daily_bp\n",
    "\n",
    "    def load_and_process_resp_data(self, resp_path):\n",
    "        resp_data = pd.read_csv(resp_path)\n",
    "        resp_data[\"DateTime\"] = pd.to_datetime(resp_data[\"DateTime\"])\n",
    "        resp_data[\"StartDate\"] = resp_data[\"DateTime\"].dt.date\n",
    "\n",
    "        daily_resp = (\n",
    "            resp_data.groupby([\"ID\", \"StartDate\"])[\"Value\"].mean().reset_index()\n",
    "        )\n",
    "        daily_resp.columns = [\"ID\", \"StartDate\", \"MeanDailyResp\"]\n",
    "\n",
    "        return daily_resp\n",
    "\n",
    "    def load_and_process_io_data(self, io_path):\n",
    "        io_data = pd.read_csv(io_path)\n",
    "        io_data[\"StartDate\"] = pd.to_datetime(io_data[\"StartDate\"]).dt.date\n",
    "\n",
    "        io_pivot = io_data.pivot_table(\n",
    "            index=[\"ID\", \"StartDate\"], columns=\"OBX\", values=\"Value\", fill_value=0\n",
    "        ).reset_index()\n",
    "\n",
    "        return io_pivot\n",
    "\n",
    "    def predict_smoothed_weight(self, patient_id, day_from_birth):\n",
    "        if patient_id in self.growth_models:\n",
    "            return self.growth_models[patient_id](day_from_birth)\n",
    "        return np.nan\n",
    "\n",
    "    def calculate_weight_derivatives(self, patient_id, day_from_birth):\n",
    "        if patient_id not in self.growth_models:\n",
    "            return np.nan, np.nan\n",
    "\n",
    "        model = self.growth_models[patient_id]\n",
    "\n",
    "        try:\n",
    "            current_weight = model(day_from_birth)\n",
    "            next_day_weight = model(day_from_birth + 1)\n",
    "            daily_gain = next_day_weight - current_weight\n",
    "\n",
    "        except Exception:\n",
    "            daily_gain = np.nan\n",
    "\n",
    "        try:\n",
    "            week_later_weight = model(day_from_birth + 1)\n",
    "            week_earlier_weight = model(day_from_birth - 6)\n",
    "            weekly_gain = (week_later_weight - week_earlier_weight) / 7\n",
    "        except Exception:\n",
    "            weekly_gain = np.nan\n",
    "\n",
    "        return daily_gain, weekly_gain\n",
    "\n",
    "    def calculate_rolling_statistics(\n",
    "        self, df, patient_col, date_col, value_col, window=7\n",
    "    ):\n",
    "        df = df.sort_values([patient_col, date_col])\n",
    "\n",
    "        df[f\"{value_col}_rolling_mean\"] = df.groupby(patient_col)[value_col].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "        df[f\"{value_col}_rolling_std\"] = df.groupby(patient_col)[value_col].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).std()\n",
    "        )\n",
    "\n",
    "        return df\n",
    "\n",
    "    def consolidate_all_data(\n",
    "        self,\n",
    "        demographics,\n",
    "        energy_data,\n",
    "        pulse_data,\n",
    "        io_data,\n",
    "        spo2_data,\n",
    "        resp_data,\n",
    "        stool_data,\n",
    "        bp_data,\n",
    "    ):\n",
    "        consolidated = demographics[[\"ID\", \"DOB\", \"GA\", \"BW\", \"Sex\"]].copy()\n",
    "        consolidated = consolidated.merge(energy_data, on=\"ID\", how=\"inner\")\n",
    "        consolidated = consolidated.merge(\n",
    "            pulse_data, on=[\"ID\", \"StartDate\"], how=\"inner\"\n",
    "        )\n",
    "        consolidated = consolidated.merge(io_data, on=[\"ID\", \"StartDate\"], how=\"inner\")\n",
    "        consolidated = consolidated.merge(\n",
    "            spo2_data, on=[\"ID\", \"StartDate\"], how=\"inner\"\n",
    "        )\n",
    "        consolidated = consolidated.merge(\n",
    "            resp_data, on=[\"ID\", \"StartDate\"], how=\"inner\"\n",
    "        )\n",
    "        consolidated = consolidated.merge(\n",
    "            stool_data, on=[\"ID\", \"StartDate\"], how=\"inner\"\n",
    "        )\n",
    "        consolidated = consolidated.merge(bp_data, on=[\"ID\", \"StartDate\"], how=\"inner\")\n",
    "\n",
    "        consolidated[\"DayFromBirthNumeric\"] = (\n",
    "            pd.to_datetime(consolidated[\"StartDate\"]) - consolidated[\"DOB\"]\n",
    "        ).dt.days.astype(float)\n",
    "\n",
    "        consolidated[\"PMADays\"] = (\n",
    "            7 * consolidated[\"GA\"] + consolidated[\"DayFromBirthNumeric\"]\n",
    "        ).astype(int)\n",
    "\n",
    "        consolidated[\"SexMInt\"] = (consolidated[\"Sex\"] == \"M\").astype(int)\n",
    "\n",
    "        consolidated[\"SmoothedWeightG\"] = consolidated.apply(\n",
    "            lambda row: self.predict_smoothed_weight(\n",
    "                row[\"ID\"], row[\"DayFromBirthNumeric\"]\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        weight_gains = consolidated.apply(\n",
    "            lambda row: self.calculate_weight_derivatives(\n",
    "                row[\"ID\"], row[\"DayFromBirthNumeric\"]\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        consolidated[\"DailyWeightGainGPerDay\"] = [x[0] for x in weight_gains]\n",
    "        consolidated[\"WeeklyWeightGainGPerDay\"] = [x[1] for x in weight_gains]\n",
    "\n",
    "        consolidated[\"DailyEnergyPerKG\"] = (\n",
    "            consolidated[\"DailyEnergy\"] * 1000 / consolidated[\"SmoothedWeightG\"]\n",
    "        )\n",
    "        consolidated[\"DailyEnergyParenteralPerKG\"] = (\n",
    "            consolidated[\"DailyEnergyParenteral\"]\n",
    "            * 1000\n",
    "            / consolidated[\"SmoothedWeightG\"]\n",
    "        )\n",
    "        consolidated[\"DailyWeightGainGPerKGPerDay\"] = (\n",
    "            consolidated[\"DailyWeightGainGPerDay\"]\n",
    "            * 1000\n",
    "            / consolidated[\"SmoothedWeightG\"]\n",
    "        )\n",
    "        consolidated[\"WeeklyWeightGainGPerKGPerDay\"] = (\n",
    "            consolidated[\"WeeklyWeightGainGPerDay\"]\n",
    "            * 1000\n",
    "            / consolidated[\"SmoothedWeightG\"]\n",
    "        )\n",
    "        consolidated[\"MeanDailyPulsePerKG\"] = (\n",
    "            consolidated[\"MeanDailyPulse\"] * 1000 / consolidated[\"SmoothedWeightG\"]\n",
    "        )\n",
    "\n",
    "        if \"DailyFluidIntake\" in consolidated.columns:\n",
    "            consolidated[\"DailyFluidIntakePerKG\"] = (\n",
    "                consolidated[\"DailyFluidIntake\"]\n",
    "                * 1000\n",
    "                / consolidated[\"SmoothedWeightG\"]\n",
    "            )\n",
    "        if \"DailyFluidOutput\" in consolidated.columns:\n",
    "            consolidated[\"DailyFluidOutputPerKG\"] = (\n",
    "                consolidated[\"DailyFluidOutput\"]\n",
    "                * 1000\n",
    "                / consolidated[\"SmoothedWeightG\"]\n",
    "            )\n",
    "\n",
    "        if \"DailyFluidIntake\" in consolidated.columns:\n",
    "            consolidated = self.calculate_rolling_statistics(\n",
    "                consolidated, \"ID\", \"StartDate\", \"DailyFluidIntake\"\n",
    "            )\n",
    "        if \"DailyFluidOutput\" in consolidated.columns:\n",
    "            consolidated = self.calculate_rolling_statistics(\n",
    "                consolidated, \"ID\", \"StartDate\", \"DailyFluidOutput\"\n",
    "            )\n",
    "\n",
    "        consolidated[\"GrowthStage\"] = np.where(\n",
    "            consolidated[\"PMADays\"] < 184,\n",
    "            1,\n",
    "            np.where(consolidated[\"PMADays\"] < 237, 2, 3),\n",
    "        )\n",
    "\n",
    "        consolidated = consolidated[consolidated[\"PMADays\"] < 280]\n",
    "\n",
    "        patient_ids = consolidated[\"ID\"].unique()\n",
    "        train_ids = np.random.choice(\n",
    "            patient_ids, size=int(len(patient_ids) * 0.5), replace=False\n",
    "        )\n",
    "        consolidated[\"Train\"] = consolidated[\"ID\"].isin(train_ids)\n",
    "\n",
    "        self.consolidated_data = consolidated\n",
    "        return consolidated\n",
    "\n",
    "    def get_normal_weekly_growth(self, ga, week_from_birth, weight_g):\n",
    "        pma = ga + week_from_birth - 1\n",
    "\n",
    "        if week_from_birth == 1:\n",
    "            return 0\n",
    "        elif pma > 34:\n",
    "            return 17 * weight_g / 1000\n",
    "        else:\n",
    "            return 30\n",
    "\n",
    "    def add_normal_growth_comparison(self):\n",
    "        if self.consolidated_data is None:\n",
    "            raise ValueError(\"No consolidated data available.\")\n",
    "\n",
    "        self.consolidated_data[\"WeekFromBirth\"] = (\n",
    "            self.consolidated_data[\"DayFromBirthNumeric\"] / 7\n",
    "        ).astype(int) + 1\n",
    "\n",
    "        self.consolidated_data[\"NormalWeeklyGrowthGPerDay\"] = (\n",
    "            self.consolidated_data.apply(\n",
    "                lambda row: self.get_normal_weekly_growth(\n",
    "                    row[\"GA\"], row[\"WeekFromBirth\"], row[\"SmoothedWeightG\"]\n",
    "                ),\n",
    "                axis=1,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return self.consolidated_data\n",
    "\n",
    "    def create_feature_list(self):\n",
    "        base_features = [\n",
    "            \"GA\",\n",
    "            \"BW\",\n",
    "            \"DailyEnergy\",\n",
    "            \"DailyEnergyParenteral\",\n",
    "            \"DayFromBirthNumeric\",\n",
    "            \"SmoothedWeightG\",\n",
    "            \"MeanDailyPulse\",\n",
    "            \"PMADays\",\n",
    "            \"SexMInt\",\n",
    "            \"DailyEnergyPerKG\",\n",
    "            \"DailyEnergyParenteralPerKG\",\n",
    "            \"MeanDailyPulsePerKG\",\n",
    "            \"MeanDailySystolicBP\",\n",
    "            \"MeanDailyDiastolicBP\",\n",
    "            \"SumDailyStoolWeight\",\n",
    "            \"MeanDailyResp\",\n",
    "            \"MeanDailySPO2\",\n",
    "        ]\n",
    "\n",
    "        if \"DailyFluidIntake\" in self.consolidated_data.columns:\n",
    "            base_features.extend(\n",
    "                [\n",
    "                    \"DailyFluidIntake\",\n",
    "                    \"DailyFluidIntakePerKG\",\n",
    "                    \"DailyFluidIntake_rolling_mean\",\n",
    "                    \"DailyFluidIntake_rolling_std\",\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        if \"DailyFluidOutput\" in self.consolidated_data.columns:\n",
    "            base_features.extend(\n",
    "                [\n",
    "                    \"DailyFluidOutput\",\n",
    "                    \"DailyFluidOutputPerKG\",\n",
    "                    \"DailyFluidOutput_rolling_mean\",\n",
    "                    \"DailyFluidOutput_rolling_std\",\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        extended_features = base_features.copy()\n",
    "        for feature in base_features:\n",
    "            if feature in self.consolidated_data.columns:\n",
    "                extended_features.append(f\"{feature}_squared\")\n",
    "                extended_features.append(f\"{feature}_sqrt\")\n",
    "\n",
    "        return extended_features\n",
    "\n",
    "    def create_polynomial_features(self):\n",
    "        if self.consolidated_data is None:\n",
    "            raise ValueError(\"No consolidated data available.\")\n",
    "\n",
    "        base_features = self.create_feature_list()\n",
    "\n",
    "        for feature in base_features:\n",
    "            if feature in self.consolidated_data.columns:\n",
    "                self.consolidated_data[f\"{feature}_squared\"] = (\n",
    "                    self.consolidated_data[feature] ** 2\n",
    "                )\n",
    "\n",
    "                self.consolidated_data[f\"{feature}_sqrt\"] = np.where(\n",
    "                    self.consolidated_data[feature] >= 0,\n",
    "                    self.consolidated_data[feature] ** 0.5,\n",
    "                    np.nan,\n",
    "                )\n",
    "\n",
    "        return self.consolidated_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97af30b2",
   "metadata": {},
   "source": [
    "This section of code is simply responsible for calling all of the top level functions found in the `GrowthDataTransformation()` class. The final consolidated csv file is built and saved here. Note that if the consolidated csv is already created, this data creation code blocks do not need to be run again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58886ac9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Valid Patients: 835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  res, _ = _lowess(y, x, x, np.ones_like(x),\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/home/walkerdavis/projects/mpsc/.venv/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n"
     ]
    }
   ],
   "source": [
    "analysis = GrowthDataTransformation()\n",
    "\n",
    "demographics = analysis.load_and_process_demographics(\n",
    "    os.path.join(DATA_ROOT, \"Demographics.csv\")\n",
    ")\n",
    "weight_data = analysis.load_and_process_weight_data(\n",
    "    os.path.join(DATA_ROOT, \"WeightObservations.csv\")\n",
    ")\n",
    "\n",
    "growth_data = analysis.create_growth_tibble(demographics, weight_data)\n",
    "smoothed_growth = analysis.fit_growth_models(growth_data)\n",
    "\n",
    "energy_data = analysis.load_and_process_energy_data(\n",
    "    os.path.join(DATA_ROOT, \"CalculatedEnergyObservations.csv\")\n",
    ")\n",
    "\n",
    "pulse_data = analysis.load_and_process_pulse_data(\n",
    "    os.path.join(DATA_ROOT, \"PulseObservations.csv\")\n",
    ")\n",
    "\n",
    "spo2_data = analysis.load_and_process_spo2_data(os.path.join(DATA_ROOT, \"SPO2Obs.csv\"))\n",
    "\n",
    "resp_data = analysis.load_and_process_resp_data(os.path.join(DATA_ROOT, \"RespObs.csv\"))\n",
    "\n",
    "\n",
    "io_data = analysis.load_and_process_io_data(os.path.join(DATA_ROOT, \"DailyIO.csv\"))\n",
    "\n",
    "stool_data = analysis.load_and_process_stool_weight_data(\n",
    "    os.path.join(DATA_ROOT, \"StoolWeightObservations.csv\")\n",
    ")\n",
    "\n",
    "bp_data = analysis.load_and_process_bp_data(\n",
    "    os.path.join(DATA_ROOT, \"BPObservations.csv\")\n",
    ")\n",
    "\n",
    "consolidated = analysis.consolidate_all_data(\n",
    "    demographics,\n",
    "    energy_data,\n",
    "    pulse_data,\n",
    "    io_data,\n",
    "    spo2_data,\n",
    "    resp_data,\n",
    "    stool_data,\n",
    "    bp_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a9c8f21",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_253282/3273377643.py:476: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_squared\"] = (\n",
      "/tmp/ipykernel_253282/3273377643.py:480: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_sqrt\"] = np.where(\n",
      "/tmp/ipykernel_253282/3273377643.py:476: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_squared\"] = (\n",
      "/tmp/ipykernel_253282/3273377643.py:480: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_sqrt\"] = np.where(\n",
      "/tmp/ipykernel_253282/3273377643.py:476: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_squared\"] = (\n",
      "/tmp/ipykernel_253282/3273377643.py:480: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_sqrt\"] = np.where(\n",
      "/tmp/ipykernel_253282/3273377643.py:476: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_squared\"] = (\n",
      "/tmp/ipykernel_253282/3273377643.py:480: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_sqrt\"] = np.where(\n",
      "/tmp/ipykernel_253282/3273377643.py:476: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_squared\"] = (\n",
      "/tmp/ipykernel_253282/3273377643.py:480: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_sqrt\"] = np.where(\n",
      "/tmp/ipykernel_253282/3273377643.py:476: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_squared\"] = (\n",
      "/tmp/ipykernel_253282/3273377643.py:480: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_sqrt\"] = np.where(\n",
      "/tmp/ipykernel_253282/3273377643.py:476: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_squared\"] = (\n",
      "/tmp/ipykernel_253282/3273377643.py:480: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_sqrt\"] = np.where(\n",
      "/tmp/ipykernel_253282/3273377643.py:476: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_squared\"] = (\n",
      "/tmp/ipykernel_253282/3273377643.py:480: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_sqrt\"] = np.where(\n",
      "/tmp/ipykernel_253282/3273377643.py:476: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_squared\"] = (\n",
      "/tmp/ipykernel_253282/3273377643.py:480: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_sqrt\"] = np.where(\n",
      "/tmp/ipykernel_253282/3273377643.py:476: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_squared\"] = (\n",
      "/tmp/ipykernel_253282/3273377643.py:480: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_sqrt\"] = np.where(\n",
      "/tmp/ipykernel_253282/3273377643.py:476: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_squared\"] = (\n",
      "/tmp/ipykernel_253282/3273377643.py:480: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_sqrt\"] = np.where(\n",
      "/tmp/ipykernel_253282/3273377643.py:476: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_squared\"] = (\n",
      "/tmp/ipykernel_253282/3273377643.py:480: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_sqrt\"] = np.where(\n",
      "/tmp/ipykernel_253282/3273377643.py:476: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_squared\"] = (\n",
      "/tmp/ipykernel_253282/3273377643.py:480: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_sqrt\"] = np.where(\n",
      "/tmp/ipykernel_253282/3273377643.py:476: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_squared\"] = (\n",
      "/tmp/ipykernel_253282/3273377643.py:480: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_sqrt\"] = np.where(\n",
      "/tmp/ipykernel_253282/3273377643.py:476: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_squared\"] = (\n",
      "/tmp/ipykernel_253282/3273377643.py:480: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_sqrt\"] = np.where(\n",
      "/tmp/ipykernel_253282/3273377643.py:476: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_squared\"] = (\n",
      "/tmp/ipykernel_253282/3273377643.py:480: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_sqrt\"] = np.where(\n",
      "/tmp/ipykernel_253282/3273377643.py:476: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_squared\"] = (\n",
      "/tmp/ipykernel_253282/3273377643.py:480: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_sqrt\"] = np.where(\n",
      "/tmp/ipykernel_253282/3273377643.py:476: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_squared\"] = (\n",
      "/tmp/ipykernel_253282/3273377643.py:480: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_sqrt\"] = np.where(\n",
      "/tmp/ipykernel_253282/3273377643.py:476: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_squared\"] = (\n",
      "/tmp/ipykernel_253282/3273377643.py:480: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_sqrt\"] = np.where(\n",
      "/tmp/ipykernel_253282/3273377643.py:476: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_squared\"] = (\n",
      "/tmp/ipykernel_253282/3273377643.py:480: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_sqrt\"] = np.where(\n",
      "/tmp/ipykernel_253282/3273377643.py:476: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_squared\"] = (\n",
      "/tmp/ipykernel_253282/3273377643.py:480: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_sqrt\"] = np.where(\n",
      "/tmp/ipykernel_253282/3273377643.py:476: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_squared\"] = (\n",
      "/tmp/ipykernel_253282/3273377643.py:480: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_sqrt\"] = np.where(\n",
      "/tmp/ipykernel_253282/3273377643.py:476: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_squared\"] = (\n",
      "/tmp/ipykernel_253282/3273377643.py:480: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_sqrt\"] = np.where(\n",
      "/tmp/ipykernel_253282/3273377643.py:476: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_squared\"] = (\n",
      "/tmp/ipykernel_253282/3273377643.py:480: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_sqrt\"] = np.where(\n",
      "/tmp/ipykernel_253282/3273377643.py:476: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_squared\"] = (\n",
      "/tmp/ipykernel_253282/3273377643.py:480: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_sqrt\"] = np.where(\n",
      "/tmp/ipykernel_253282/3273377643.py:476: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_squared\"] = (\n",
      "/tmp/ipykernel_253282/3273377643.py:480: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_sqrt\"] = np.where(\n",
      "/tmp/ipykernel_253282/3273377643.py:476: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_squared\"] = (\n",
      "/tmp/ipykernel_253282/3273377643.py:480: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_sqrt\"] = np.where(\n",
      "/tmp/ipykernel_253282/3273377643.py:476: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_squared\"] = (\n",
      "/tmp/ipykernel_253282/3273377643.py:480: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_sqrt\"] = np.where(\n",
      "/tmp/ipykernel_253282/3273377643.py:476: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_squared\"] = (\n",
      "/tmp/ipykernel_253282/3273377643.py:480: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_sqrt\"] = np.where(\n",
      "/tmp/ipykernel_253282/3273377643.py:476: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_squared\"] = (\n",
      "/tmp/ipykernel_253282/3273377643.py:480: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.consolidated_data[f\"{feature}_sqrt\"] = np.where(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>DOB</th>\n",
       "      <th>GA</th>\n",
       "      <th>BW</th>\n",
       "      <th>Sex</th>\n",
       "      <th>StartDate</th>\n",
       "      <th>DailyEnergyEnteral</th>\n",
       "      <th>DailyEnergyParenteral</th>\n",
       "      <th>DailyEnergy</th>\n",
       "      <th>MeanDailyPulse</th>\n",
       "      <th>...</th>\n",
       "      <th>DailyFluidOutputPerKG_sqrt_squared</th>\n",
       "      <th>DailyFluidOutputPerKG_sqrt_sqrt</th>\n",
       "      <th>DailyFluidOutput_rolling_mean_squared_squared</th>\n",
       "      <th>DailyFluidOutput_rolling_mean_squared_sqrt</th>\n",
       "      <th>DailyFluidOutput_rolling_mean_sqrt_squared</th>\n",
       "      <th>DailyFluidOutput_rolling_mean_sqrt_sqrt</th>\n",
       "      <th>DailyFluidOutput_rolling_std_squared_squared</th>\n",
       "      <th>DailyFluidOutput_rolling_std_squared_sqrt</th>\n",
       "      <th>DailyFluidOutput_rolling_std_sqrt_squared</th>\n",
       "      <th>DailyFluidOutput_rolling_std_sqrt_sqrt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25258</th>\n",
       "      <td>Lurie_000008</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>38.0</td>\n",
       "      <td>2620</td>\n",
       "      <td>M</td>\n",
       "      <td>2023-01-07</td>\n",
       "      <td>43.94</td>\n",
       "      <td>172.59</td>\n",
       "      <td>216.53</td>\n",
       "      <td>121.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>41.982975</td>\n",
       "      <td>2.545472</td>\n",
       "      <td>3.224179e+08</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>3.402328</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25259</th>\n",
       "      <td>Lurie_000008</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>38.0</td>\n",
       "      <td>2620</td>\n",
       "      <td>M</td>\n",
       "      <td>2023-01-08</td>\n",
       "      <td>27.04</td>\n",
       "      <td>290.75</td>\n",
       "      <td>317.79</td>\n",
       "      <td>106.619048</td>\n",
       "      <td>...</td>\n",
       "      <td>140.580093</td>\n",
       "      <td>3.443348</td>\n",
       "      <td>6.975757e+09</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>4.123106</td>\n",
       "      <td>2.308802e+09</td>\n",
       "      <td>219.203102</td>\n",
       "      <td>219.203102</td>\n",
       "      <td>3.847793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25260</th>\n",
       "      <td>Lurie_000008</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>38.0</td>\n",
       "      <td>2620</td>\n",
       "      <td>M</td>\n",
       "      <td>2023-01-09</td>\n",
       "      <td>54.08</td>\n",
       "      <td>288.30</td>\n",
       "      <td>342.38</td>\n",
       "      <td>126.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>166.960508</td>\n",
       "      <td>3.594624</td>\n",
       "      <td>1.807531e+10</td>\n",
       "      <td>366.666667</td>\n",
       "      <td>366.666667</td>\n",
       "      <td>4.375905</td>\n",
       "      <td>1.774207e+09</td>\n",
       "      <td>205.234825</td>\n",
       "      <td>205.234825</td>\n",
       "      <td>3.784973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25261</th>\n",
       "      <td>Lurie_000008</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>38.0</td>\n",
       "      <td>2620</td>\n",
       "      <td>M</td>\n",
       "      <td>2023-01-10</td>\n",
       "      <td>47.32</td>\n",
       "      <td>302.52</td>\n",
       "      <td>349.84</td>\n",
       "      <td>150.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>87.341755</td>\n",
       "      <td>3.057071</td>\n",
       "      <td>1.378086e+10</td>\n",
       "      <td>342.625000</td>\n",
       "      <td>342.625000</td>\n",
       "      <td>4.302340</td>\n",
       "      <td>9.237281e+08</td>\n",
       "      <td>174.335584</td>\n",
       "      <td>174.335584</td>\n",
       "      <td>3.633679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25262</th>\n",
       "      <td>Lurie_000008</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>38.0</td>\n",
       "      <td>2620</td>\n",
       "      <td>M</td>\n",
       "      <td>2023-01-11</td>\n",
       "      <td>67.60</td>\n",
       "      <td>349.52</td>\n",
       "      <td>417.12</td>\n",
       "      <td>134.142857</td>\n",
       "      <td>...</td>\n",
       "      <td>178.41335</td>\n",
       "      <td>3.654743</td>\n",
       "      <td>2.164830e+10</td>\n",
       "      <td>383.580000</td>\n",
       "      <td>383.580000</td>\n",
       "      <td>4.425517</td>\n",
       "      <td>9.722692e+08</td>\n",
       "      <td>176.582083</td>\n",
       "      <td>176.582083</td>\n",
       "      <td>3.645328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12567</th>\n",
       "      <td>Lurie_001800</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>23.0</td>\n",
       "      <td>700</td>\n",
       "      <td>M</td>\n",
       "      <td>2024-04-08</td>\n",
       "      <td>337.58</td>\n",
       "      <td>21.27</td>\n",
       "      <td>358.85</td>\n",
       "      <td>141.833333</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.026361e+09</td>\n",
       "      <td>251.900000</td>\n",
       "      <td>251.900000</td>\n",
       "      <td>3.983887</td>\n",
       "      <td>1.488210e+08</td>\n",
       "      <td>110.450079</td>\n",
       "      <td>110.450079</td>\n",
       "      <td>3.241839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15298</th>\n",
       "      <td>Lurie_001801</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>39.0</td>\n",
       "      <td>3665</td>\n",
       "      <td>M</td>\n",
       "      <td>2025-01-02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>82.35</td>\n",
       "      <td>82.35</td>\n",
       "      <td>90.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.677722e+07</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11961</th>\n",
       "      <td>Lurie_001802</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3970</td>\n",
       "      <td>F</td>\n",
       "      <td>2025-01-14</td>\n",
       "      <td>94.68</td>\n",
       "      <td>0.00</td>\n",
       "      <td>94.68</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.059118e+08</td>\n",
       "      <td>163.000000</td>\n",
       "      <td>163.000000</td>\n",
       "      <td>3.573114</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11962</th>\n",
       "      <td>Lurie_001802</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3970</td>\n",
       "      <td>F</td>\n",
       "      <td>2025-01-15</td>\n",
       "      <td>407.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>407.13</td>\n",
       "      <td>166.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.023928e+09</td>\n",
       "      <td>234.500000</td>\n",
       "      <td>234.500000</td>\n",
       "      <td>3.913233</td>\n",
       "      <td>1.045404e+08</td>\n",
       "      <td>101.116270</td>\n",
       "      <td>101.116270</td>\n",
       "      <td>3.171066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21139</th>\n",
       "      <td>Lurie_001803</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>37.0</td>\n",
       "      <td>3120</td>\n",
       "      <td>M</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.98</td>\n",
       "      <td>12.98</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.211736e+07</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>2.771488</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7761 rows × 188 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID        DOB    GA    BW Sex   StartDate  \\\n",
       "25258  Lurie_000008 2023-01-01  38.0  2620   M  2023-01-07   \n",
       "25259  Lurie_000008 2023-01-01  38.0  2620   M  2023-01-08   \n",
       "25260  Lurie_000008 2023-01-01  38.0  2620   M  2023-01-09   \n",
       "25261  Lurie_000008 2023-01-01  38.0  2620   M  2023-01-10   \n",
       "25262  Lurie_000008 2023-01-01  38.0  2620   M  2023-01-11   \n",
       "...             ...        ...   ...   ...  ..         ...   \n",
       "12567  Lurie_001800 2024-01-01  23.0   700   M  2024-04-08   \n",
       "15298  Lurie_001801 2025-01-01  39.0  3665   M  2025-01-02   \n",
       "11961  Lurie_001802 2025-01-01  35.0  3970   F  2025-01-14   \n",
       "11962  Lurie_001802 2025-01-01  35.0  3970   F  2025-01-15   \n",
       "21139  Lurie_001803 2025-01-01  37.0  3120   M  2025-01-01   \n",
       "\n",
       "       DailyEnergyEnteral  DailyEnergyParenteral  DailyEnergy  MeanDailyPulse  \\\n",
       "25258               43.94                 172.59       216.53      121.400000   \n",
       "25259               27.04                 290.75       317.79      106.619048   \n",
       "25260               54.08                 288.30       342.38      126.666667   \n",
       "25261               47.32                 302.52       349.84      150.400000   \n",
       "25262               67.60                 349.52       417.12      134.142857   \n",
       "...                   ...                    ...          ...             ...   \n",
       "12567              337.58                  21.27       358.85      141.833333   \n",
       "15298                0.00                  82.35        82.35       90.333333   \n",
       "11961               94.68                   0.00        94.68      160.000000   \n",
       "11962              407.13                   0.00       407.13      166.333333   \n",
       "21139                0.00                  12.98        12.98      112.000000   \n",
       "\n",
       "       ...  DailyFluidOutputPerKG_sqrt_squared  \\\n",
       "25258  ...                           41.982975   \n",
       "25259  ...                          140.580093   \n",
       "25260  ...                          166.960508   \n",
       "25261  ...                           87.341755   \n",
       "25262  ...                           178.41335   \n",
       "...    ...                                 ...   \n",
       "12567  ...                                 NaN   \n",
       "15298  ...                                 NaN   \n",
       "11961  ...                                 NaN   \n",
       "11962  ...                                 NaN   \n",
       "21139  ...                                 NaN   \n",
       "\n",
       "       DailyFluidOutputPerKG_sqrt_sqrt  \\\n",
       "25258                         2.545472   \n",
       "25259                         3.443348   \n",
       "25260                         3.594624   \n",
       "25261                         3.057071   \n",
       "25262                         3.654743   \n",
       "...                                ...   \n",
       "12567                              NaN   \n",
       "15298                              NaN   \n",
       "11961                              NaN   \n",
       "11962                              NaN   \n",
       "21139                              NaN   \n",
       "\n",
       "       DailyFluidOutput_rolling_mean_squared_squared  \\\n",
       "25258                                   3.224179e+08   \n",
       "25259                                   6.975757e+09   \n",
       "25260                                   1.807531e+10   \n",
       "25261                                   1.378086e+10   \n",
       "25262                                   2.164830e+10   \n",
       "...                                              ...   \n",
       "12567                                   4.026361e+09   \n",
       "15298                                   1.677722e+07   \n",
       "11961                                   7.059118e+08   \n",
       "11962                                   3.023928e+09   \n",
       "21139                                   1.211736e+07   \n",
       "\n",
       "       DailyFluidOutput_rolling_mean_squared_sqrt  \\\n",
       "25258                                  134.000000   \n",
       "25259                                  289.000000   \n",
       "25260                                  366.666667   \n",
       "25261                                  342.625000   \n",
       "25262                                  383.580000   \n",
       "...                                           ...   \n",
       "12567                                  251.900000   \n",
       "15298                                   64.000000   \n",
       "11961                                  163.000000   \n",
       "11962                                  234.500000   \n",
       "21139                                   59.000000   \n",
       "\n",
       "       DailyFluidOutput_rolling_mean_sqrt_squared  \\\n",
       "25258                                  134.000000   \n",
       "25259                                  289.000000   \n",
       "25260                                  366.666667   \n",
       "25261                                  342.625000   \n",
       "25262                                  383.580000   \n",
       "...                                           ...   \n",
       "12567                                  251.900000   \n",
       "15298                                   64.000000   \n",
       "11961                                  163.000000   \n",
       "11962                                  234.500000   \n",
       "21139                                   59.000000   \n",
       "\n",
       "       DailyFluidOutput_rolling_mean_sqrt_sqrt  \\\n",
       "25258                                 3.402328   \n",
       "25259                                 4.123106   \n",
       "25260                                 4.375905   \n",
       "25261                                 4.302340   \n",
       "25262                                 4.425517   \n",
       "...                                        ...   \n",
       "12567                                 3.983887   \n",
       "15298                                 2.828427   \n",
       "11961                                 3.573114   \n",
       "11962                                 3.913233   \n",
       "21139                                 2.771488   \n",
       "\n",
       "       DailyFluidOutput_rolling_std_squared_squared  \\\n",
       "25258                                           NaN   \n",
       "25259                                  2.308802e+09   \n",
       "25260                                  1.774207e+09   \n",
       "25261                                  9.237281e+08   \n",
       "25262                                  9.722692e+08   \n",
       "...                                             ...   \n",
       "12567                                  1.488210e+08   \n",
       "15298                                           NaN   \n",
       "11961                                           NaN   \n",
       "11962                                  1.045404e+08   \n",
       "21139                                           NaN   \n",
       "\n",
       "       DailyFluidOutput_rolling_std_squared_sqrt  \\\n",
       "25258                                        NaN   \n",
       "25259                                 219.203102   \n",
       "25260                                 205.234825   \n",
       "25261                                 174.335584   \n",
       "25262                                 176.582083   \n",
       "...                                          ...   \n",
       "12567                                 110.450079   \n",
       "15298                                        NaN   \n",
       "11961                                        NaN   \n",
       "11962                                 101.116270   \n",
       "21139                                        NaN   \n",
       "\n",
       "       DailyFluidOutput_rolling_std_sqrt_squared  \\\n",
       "25258                                        NaN   \n",
       "25259                                 219.203102   \n",
       "25260                                 205.234825   \n",
       "25261                                 174.335584   \n",
       "25262                                 176.582083   \n",
       "...                                          ...   \n",
       "12567                                 110.450079   \n",
       "15298                                        NaN   \n",
       "11961                                        NaN   \n",
       "11962                                 101.116270   \n",
       "21139                                        NaN   \n",
       "\n",
       "       DailyFluidOutput_rolling_std_sqrt_sqrt  \n",
       "25258                                     NaN  \n",
       "25259                                3.847793  \n",
       "25260                                3.784973  \n",
       "25261                                3.633679  \n",
       "25262                                3.645328  \n",
       "...                                       ...  \n",
       "12567                                3.241839  \n",
       "15298                                     NaN  \n",
       "11961                                     NaN  \n",
       "11962                                3.171066  \n",
       "21139                                     NaN  \n",
       "\n",
       "[7761 rows x 188 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data = analysis.add_normal_growth_comparison()\n",
    "final_data = analysis.create_polynomial_features()\n",
    "\n",
    "final_data.to_csv(os.path.join(DATA_ROOT, \"Consolidated.csv\"), index=False)\n",
    "\n",
    "final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669c474a",
   "metadata": {},
   "source": [
    "## Train and Test Data Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37e6e83",
   "metadata": {},
   "source": [
    "Loading the consolidated csv file and dropping any missing values in `WeeklyWeightGainGPerKGPerDay`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40a8c4be",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_ROOT, \"Consolidated.csv\"))\n",
    "\n",
    "df = df.dropna(subset=[\"WeeklyWeightGainGPerKGPerDay\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceb2646",
   "metadata": {},
   "source": [
    "1. We drop all values that are derivatives of `WeeklyWeightGainGPerKGPerDay` to avoid data leakage. Data is then split into X and y.\n",
    "2. For each categorical/text column:\n",
    "    - Missing values are filled with \"Missing\" (as a string label).\n",
    "    - Each unique value is replaced with an integer (e.g., \"A\", \"B\", \"Missing\" → 0, 1, 2).\n",
    "3. For each numeric feature, any missing values are filled in with the median value of that column.\n",
    "4. All values of X are limited to between $-1000000$ and $1000000$. This is to stop massive values from overflowing.\n",
    "5. Splits the data into 80% train, 20% test. A seed is set to ensure the split stays constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e12f8fa4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop(\n",
    "    columns=[\n",
    "        \"WeeklyWeightGainGPerDay\",\n",
    "        \"WeeklyWeightGainGPerKGPerDay\",\n",
    "        \"DailyWeightGainGPerDay\",\n",
    "        \"DailyWeightGainGPerKGPerDay\",\n",
    "    ]\n",
    ")\n",
    "y = df[\"WeeklyWeightGainGPerKGPerDay\"]\n",
    "\n",
    "for col in X.select_dtypes(include=[\"object\", \"category\"]).columns:\n",
    "    X[col] = X[col].fillna(\"Missing\")\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "\n",
    "for col in X.select_dtypes(include=[\"float64\", \"int64\"]).columns:\n",
    "    X[col] = X[col].fillna(X[col].median())\n",
    "\n",
    "X = X.clip(lower=-1e6, upper=1e6)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d9bd58",
   "metadata": {},
   "source": [
    "A helper function that returns Mean Absolute Error, Mean Squared Error, and $R^2$ value for a given prediction and ground truth vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71c307e5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def model_results(preds, y_test):\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    mse = mean_squared_error(y_test, preds)\n",
    "    r2 = r2_score(y_test, preds)\n",
    "\n",
    "    return f\"MAE: {mae} | MSE: {mse} | r^2: {r2}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f91d7d1",
   "metadata": {},
   "source": [
    "An xgboost regressor model is trained using a random search with 3 fold cross validation. Note that the commented out blcok uses a Grid Search instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d135898",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 128 candidates, totalling 384 fits\n",
      "[CV] END colsample_bytree=0.9, gamma=5, learning_rate=0.3, max_depth=9, n_estimators=500, subsample=1.0; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.9, gamma=5, learning_rate=0.3, max_depth=9, n_estimators=500, subsample=1.0; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.9, gamma=5, learning_rate=0.3, max_depth=9, n_estimators=500, subsample=1.0; total time=   3.0s\n",
      "[CV] END colsample_bytree=1.0, gamma=0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, gamma=0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.3s\n",
      "[CV] END colsample_bytree=1.0, gamma=0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   1.2s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[33;03mgrid_search = GridSearchCV(\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[33;03m    estimator=model,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m \u001b[33;03mprint(model_results(preds=y_pred, y_test=y_test))\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     29\u001b[39m random_search = RandomizedSearchCV(\n\u001b[32m     30\u001b[39m     estimator=model,\n\u001b[32m     31\u001b[39m     param_distributions=param_grid,\n\u001b[32m   (...)\u001b[39m\u001b[32m     36\u001b[39m     n_jobs=\u001b[32m1\u001b[39m,\n\u001b[32m     37\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[43mrandom_search\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest parameters:\u001b[39m\u001b[33m\"\u001b[39m, random_search.best_params_)\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest CV score (negative MSE):\u001b[39m\u001b[33m\"\u001b[39m, random_search.best_score_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/mpsc/.venv/lib/python3.12/site-packages/sklearn/base.py:1363\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1356\u001b[39m     estimator._validate_params()\n\u001b[32m   1358\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1359\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1360\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1361\u001b[39m     )\n\u001b[32m   1362\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/mpsc/.venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1051\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1045\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1046\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1047\u001b[39m     )\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1055\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/mpsc/.venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1992\u001b[39m, in \u001b[36mRandomizedSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1990\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1991\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1992\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1993\u001b[39m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1994\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\n\u001b[32m   1995\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1996\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/mpsc/.venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:997\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    993\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    994\u001b[39m         )\n\u001b[32m    995\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1017\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1018\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1019\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1020\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/mpsc/.venv/lib/python3.12/site-packages/sklearn/utils/parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/mpsc/.venv/lib/python3.12/site-packages/joblib/parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/mpsc/.venv/lib/python3.12/site-packages/joblib/parallel.py:1914\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1916\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/mpsc/.venv/lib/python3.12/site-packages/sklearn/utils/parallel.py:147\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config), warnings.catch_warnings():\n\u001b[32m    146\u001b[39m     warnings.filters = warning_filters\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/mpsc/.venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:859\u001b[39m, in \u001b[36m_fit_and_score\u001b[39m\u001b[34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[39m\n\u001b[32m    857\u001b[39m         estimator.fit(X_train, **fit_params)\n\u001b[32m    858\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m859\u001b[39m         \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    862\u001b[39m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[32m    863\u001b[39m     fit_time = time.time() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/mpsc/.venv/lib/python3.12/site-packages/xgboost/core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/mpsc/.venv/lib/python3.12/site-packages/xgboost/sklearn.py:1247\u001b[39m, in \u001b[36mXGBModel.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1245\u001b[39m     obj = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1247\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1249\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1250\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1259\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1261\u001b[39m \u001b[38;5;28mself\u001b[39m._set_evaluation_result(evals_result)\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/mpsc/.venv/lib/python3.12/site-packages/xgboost/core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/mpsc/.venv/lib/python3.12/site-packages/xgboost/training.py:183\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[43mbst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/mpsc/.venv/lib/python3.12/site-packages/xgboost/core.py:2247\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, dtrain, iteration, fobj)\u001b[39m\n\u001b[32m   2243\u001b[39m \u001b[38;5;28mself\u001b[39m._assign_dmatrix_features(dtrain)\n\u001b[32m   2245\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2246\u001b[39m     _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m2247\u001b[39m         \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2250\u001b[39m     )\n\u001b[32m   2251\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2252\u001b[39m     pred = \u001b[38;5;28mself\u001b[39m.predict(dtrain, output_margin=\u001b[38;5;28;01mTrue\u001b[39;00m, training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "param_grid: dict[str, Union[list[int], list[float]]] = {\n",
    "    \"n_estimators\": [100, 300, 500],\n",
    "    \"max_depth\": [3, 6, 9],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.3],\n",
    "    \"subsample\": [0.7, 0.9, 1.0],\n",
    "    \"colsample_bytree\": [0.7, 0.9, 1.0],\n",
    "    \"gamma\": [0, 1, 5],\n",
    "}\n",
    "\n",
    "model = xg.XGBRegressor(objective=\"reg:squarederror\", seed=42)\n",
    "\"\"\"\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    verbose=2,\n",
    "    n_jobs=1,\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best CV score (negative MSE):\", grid_search.best_score_)\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print(model_results(preds=y_pred, y_test=y_test))\n",
    "\"\"\"\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=128,\n",
    "    cv=3,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    verbose=2,\n",
    "    n_jobs=1,\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "print(\"Best CV score (negative MSE):\", random_search.best_score_)\n",
    "\n",
    "y_pred = random_search.predict(X_test)\n",
    "print(model_results(preds=y_pred, y_test=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed502b2d",
   "metadata": {},
   "source": [
    "Listing feature importances for the trained random search model. This is used to ID features with the strongest impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeb9867",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "importances = pd.Series(\n",
    "    random_search.best_estimator_.feature_importances_, index=X_train.columns\n",
    ").sort_values(ascending=False)\n",
    "importances.head(29)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e75f58b",
   "metadata": {},
   "source": [
    "I tried to replicate the graph shown from your presentation, and as shown below, I was unsuccessful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ececee90",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df = X_test.copy()\n",
    "df[\"WeeklyWeightGainPerKgPerDay\"] = y_test\n",
    "df[\"GA * 7 + DayFromBirthNumeric\"] = df[\"GA\"] * 7 + df[\"DayFromBirthNumeric\"]\n",
    "\n",
    "n_patients = 128\n",
    "patient_ids = df[\"ID\"].unique()[:n_patients]\n",
    "df_subset = df[df[\"ID\"].isin(patient_ids)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "color_cycle = plt.cm.tab20(np.linspace(0, 1, n_patients))\n",
    "patient_colors = {pid: color_cycle[i] for i, pid in enumerate(patient_ids)}\n",
    "\n",
    "\n",
    "for patient_id, group in df_subset.groupby(\"ID\"):\n",
    "    color = patient_colors[patient_id]\n",
    "\n",
    "    x_vals = group[\"GA * 7 + DayFromBirthNumeric\"].values\n",
    "    y_real = group[\"WeeklyWeightGainPerKgPerDay\"].values\n",
    "\n",
    "    model_X = X_test.loc[group.index]\n",
    "    y_pred = random_search.predict(model_X)\n",
    "\n",
    "    ax.scatter(x_vals, y_real, s=22, alpha=0.7, color=color)\n",
    "\n",
    "    for i in range(len(x_vals)):\n",
    "        x_head = x_vals[i]\n",
    "        y_head = y_real[i]\n",
    "        pred_gain = y_pred[i]\n",
    "\n",
    "        x_tail = x_head + 7\n",
    "        y_tail = y_head + pred_gain * 7\n",
    "\n",
    "        ax.plot([x_head, x_tail], [y_head, y_tail], color=color, linewidth=1.7)\n",
    "\n",
    "ax.set_xlabel(\"GA * 7 + DayFromBirthNumeric\", fontsize=14)\n",
    "ax.set_ylabel(\"WeeklyWeightGainGPerKGPerDay\", fontsize=14)\n",
    "ax.set_title(\n",
    "    f\"First {n_patients} patients: real values (dots) & model weekly vector (lines)\",\n",
    "    fontsize=15,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2991cd",
   "metadata": {},
   "source": [
    "## Training using Optuna\n",
    "This code block uses Optuna, an optimization library to train the XGB model. The model is tested 1000 times with different hyperparameters, and the best model is chosen at the end. The process can be stopped at any time, and the best model will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00efd1bb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def objective_func(trial: optuna.Trial):\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 1000)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 10, 50)\n",
    "    min_child_weight = trial.suggest_int(\"min_child_weight\", 1, 32)\n",
    "    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0, 1)\n",
    "    objective = trial.suggest_categorical(\n",
    "        \"objective\",\n",
    "        [\n",
    "            \"reg:squarederror\",\n",
    "            \"reg:absoluteerror\",\n",
    "        ],\n",
    "    )\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.005, 0.25)\n",
    "\n",
    "    model = xg.XGBRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_child_weight=min_child_weight,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        objective=objective,\n",
    "        learning_rate=learning_rate,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    score = r2_score(y_test, y_pred)\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\", sampler=optuna.samplers.RandomSampler(seed=42)\n",
    ")\n",
    "study.optimize(objective_func, n_trials=1000)\n",
    "\n",
    "print(\"Best Trial\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(f\"Value: {trial.value}\")\n",
    "\n",
    "print(\"Params:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297519a9",
   "metadata": {},
   "source": [
    "This block prints the statistics for the best model found using Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51826f2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "\n",
    "best_n_estimators = best_params[\"n_estimators\"]\n",
    "best_max_depth = best_params[\"max_depth\"]\n",
    "best_min_child_weight = best_params[\"min_child_weight\"]\n",
    "best_colsample_bytree = best_params[\"colsample_bytree\"]\n",
    "best_objective = best_params[\"objective\"]\n",
    "best_learning_rate = best_params[\"learning_rate\"]\n",
    "\n",
    "best_model = xg.XGBRegressor(\n",
    "    n_estimators=best_n_estimators,\n",
    "    max_depth=best_max_depth,\n",
    "    min_child_weight=best_min_child_weight,\n",
    "    colsample_bytree=best_colsample_bytree,\n",
    "    objective=best_objective,\n",
    "    learning_rate=best_learning_rate,\n",
    ")\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_xgb_optuna_200_trials = best_model.predict(X_test)\n",
    "\n",
    "model_results(y_pred_xgb_optuna_200_trials, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
